{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "Lecture1_intro_to_ml.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "9FxqGMk-IEmC",
        "MJilxLa4LETk"
      ],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CharlesPoletowin/YCBS-273/blob/master/Lecture1_intro_to_ml.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nE33IvZIEl9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FxqGMk-IEmC",
        "colab_type": "text"
      },
      "source": [
        "Disclaimer: This notebook is modified from the following notebooks:\n",
        "- https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/_downloads/tensor_tutorial.ipynb#scrollTo=VswH-KuA8iIs\n",
        "- https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/_downloads/autograd_tutorial.ipynb\n",
        "- https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/_downloads/nn_tutorial.ipynb#scrollTo=N29T2vcqOnVU\n",
        "\n",
        "What is PyTorch?\n",
        "================\n",
        "\n",
        "It’s a Python-based scientific computing package targeted at two sets of\n",
        "audiences:\n",
        "\n",
        "-  A replacement for NumPy to use the power of GPUs\n",
        "-  a deep learning research platform that provides maximum flexibility\n",
        "   and speed\n",
        "\n",
        "Getting Started\n",
        "---------------\n",
        "\n",
        "Tensors\n",
        "----------\n",
        "\n",
        "Tensors are similar to NumPy’s ndarrays, with the addition being that\n",
        "Tensors can also be used on a GPU to accelerate computing.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETEKHNS_IEmD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYhD-tS7IEmF",
        "colab_type": "text"
      },
      "source": [
        "Construct a 5x3 matrix, uninitialized:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRhnDVu2IEmG",
        "colab_type": "code",
        "outputId": "3604ff8e-4ff7-41e7-d081-f3fa5d1db3f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "source": [
        "x = torch.empty(5, 3)\n",
        "print(x)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[8.5947e-36, 0.0000e+00, 3.3631e-44],\n",
            "        [0.0000e+00,        nan, 0.0000e+00],\n",
            "        [1.1578e+27, 1.1362e+30, 7.1547e+22],\n",
            "        [4.5828e+30, 1.2121e+04, 7.1846e+22],\n",
            "        [9.2198e-39, 0.0000e+00, 0.0000e+00]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9i82ik1lIEmJ",
        "colab_type": "text"
      },
      "source": [
        "Construct a randomly initialized matrix:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_mzKOFmIEmK",
        "colab_type": "code",
        "outputId": "53ee62f7-d828-49c4-f73f-16a87df075c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "source": [
        "x = torch.rand(5, 3)\n",
        "print(x)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.9402, 0.7386, 0.1908],\n",
            "        [0.5239, 0.5900, 0.4439],\n",
            "        [0.8486, 0.8946, 0.7101],\n",
            "        [0.6064, 0.7136, 0.3344],\n",
            "        [0.9959, 0.7388, 0.0531]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksjGntU5IEmN",
        "colab_type": "text"
      },
      "source": [
        "Construct a matrix filled zeros and of dtype long:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1I3f2UaeIEmQ",
        "colab_type": "code",
        "outputId": "2f99d8b9-b297-4626-8380-4182f029fe9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "source": [
        "x = torch.zeros(5, 3, dtype=torch.long)\n",
        "print(x)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0, 0, 0],\n",
            "        [0, 0, 0],\n",
            "        [0, 0, 0],\n",
            "        [0, 0, 0],\n",
            "        [0, 0, 0]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qRenUWcIEmS",
        "colab_type": "text"
      },
      "source": [
        "Construct a tensor directly from data:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rY6xFX5IEmT",
        "colab_type": "code",
        "outputId": "a062213e-bac8-4f85-ee7d-0635cfc8a3fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "x = torch.tensor([5.5, 3])\n",
        "print(x)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([5.5000, 3.0000])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNnUCSCUIEmW",
        "colab_type": "text"
      },
      "source": [
        "or create a tensor based on an existing tensor. These methods\n",
        "will reuse properties of the input tensor, e.g. dtype, unless\n",
        "new values are provided by user\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nIlWS8VIEmX",
        "colab_type": "code",
        "outputId": "9d751240-ce17-4b44-b870-28fd9551ee1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        }
      },
      "source": [
        "x = x.new_ones(5, 3, dtype=torch.double)      # new_* methods take in sizes\n",
        "print(x)\n",
        "print('-----------')\n",
        "x = torch.randn_like(x, dtype=torch.float)    # override dtype!\n",
        "print(x)                                      # result has the same size"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.],\n",
            "        [1., 1., 1.],\n",
            "        [1., 1., 1.],\n",
            "        [1., 1., 1.]], dtype=torch.float64)\n",
            "-----------\n",
            "tensor([[ 1.8923,  0.6052,  0.2544],\n",
            "        [ 1.1650, -0.1624,  1.1122],\n",
            "        [ 1.7084,  0.4722,  0.1196],\n",
            "        [-0.4437,  1.1565, -0.8016],\n",
            "        [-1.0711,  2.2638,  0.4137]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5i3xGOALIEmZ",
        "colab_type": "text"
      },
      "source": [
        "Get its size:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RKWNLnDIEma",
        "colab_type": "code",
        "outputId": "01771aa5-f0ee-4f48-817d-38f9a9a43456",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(x.size(), x.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([5, 3]) torch.Size([5, 3])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zlfAh3NIEmd",
        "colab_type": "text"
      },
      "source": [
        "<div class=\"alert alert-info\"><h4>Note</h4><p>``torch.Size`` is in fact a tuple, so it supports all tuple operations.</p></div>\n",
        "\n",
        "Operations: \n",
        "There are multiple syntaxes for operations. In the following\n",
        "example, we will take a look at the addition operation.\n",
        "\n",
        "Addition: syntax 1\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llP_0G4pIEme",
        "colab_type": "code",
        "outputId": "2b57fe36-9432-413f-d72b-ac217fcb1114",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "source": [
        "y = torch.rand(5, 3)\n",
        "print(x + y)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 2.8878,  0.9037,  0.5156],\n",
            "        [ 1.2948,  0.0757,  1.1434],\n",
            "        [ 2.0705,  0.6726,  0.5657],\n",
            "        [-0.2314,  1.9233,  0.1088],\n",
            "        [-0.6687,  2.3718,  0.8878]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6wHQ3gZIEmh",
        "colab_type": "text"
      },
      "source": [
        "Addition: syntax 2\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xUVDptPIEmh",
        "colab_type": "code",
        "outputId": "8e88f5d4-333f-4d63-b56c-c08502a27631",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "source": [
        "print(torch.add(x, y))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 2.8878,  0.9037,  0.5156],\n",
            "        [ 1.2948,  0.0757,  1.1434],\n",
            "        [ 2.0705,  0.6726,  0.5657],\n",
            "        [-0.2314,  1.9233,  0.1088],\n",
            "        [-0.6687,  2.3718,  0.8878]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yyBMQhBIEmk",
        "colab_type": "text"
      },
      "source": [
        "Addition: providing an output tensor as argument\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhSZiFLOIEml",
        "colab_type": "code",
        "outputId": "46679fc1-2664-4d11-9850-8e52ab4e4a85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "source": [
        "result = torch.empty(5, 3)\n",
        "torch.add(x, y, out=result)\n",
        "print(result)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 2.8878,  0.9037,  0.5156],\n",
            "        [ 1.2948,  0.0757,  1.1434],\n",
            "        [ 2.0705,  0.6726,  0.5657],\n",
            "        [-0.2314,  1.9233,  0.1088],\n",
            "        [-0.6687,  2.3718,  0.8878]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39ly5wJJIEmo",
        "colab_type": "text"
      },
      "source": [
        "Addition: in-place\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ainyzIBOIEmp",
        "colab_type": "code",
        "outputId": "b38affc4-626d-40ef-940f-8889f8eeeafa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "source": [
        "# adds x to y\n",
        "y.add_(x)\n",
        "print(y)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 2.8878,  0.9037,  0.5156],\n",
            "        [ 1.2948,  0.0757,  1.1434],\n",
            "        [ 2.0705,  0.6726,  0.5657],\n",
            "        [-0.2314,  1.9233,  0.1088],\n",
            "        [-0.6687,  2.3718,  0.8878]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1tELDD-IEmr",
        "colab_type": "text"
      },
      "source": [
        "<div class=\"alert alert-info\"><h4>Note</h4><p>Any operation that mutates a tensor in-place is post-fixed with an ``_``.\n",
        "    For example: ``x.copy_(y)``, ``x.t_()``, will change ``x``.</p></div>\n",
        "\n",
        "You can use standard NumPy-like indexing with all bells and whistles!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XjdNwIcjIEmr",
        "colab_type": "code",
        "outputId": "10de15fe-a22f-4d58-de85-55bc273477e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(x[:, 1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([ 0.6052, -0.1624,  0.4722,  1.1565,  2.2638])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxpulSKAIEmu",
        "colab_type": "text"
      },
      "source": [
        "Resizing: If you want to resize/reshape tensor, you can use ``torch.view``:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2kHc8a8IEmv",
        "colab_type": "code",
        "outputId": "c5d4d9ec-0a81-435e-e52e-c6f08cb16447",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "x = torch.randn(4, 4)\n",
        "y = x.view(16)\n",
        "z = x.view(-1, 8)  # the size -1 is inferred from other dimensions\n",
        "print(x.size(), y.size(), z.size())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zkh4TevAIEmw",
        "colab_type": "text"
      },
      "source": [
        "If you have a one element tensor, use ``.item()`` to get the value as a\n",
        "Python number\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3WUDehrJIEmx",
        "colab_type": "code",
        "outputId": "c930bb75-1a39-4e9e-fbe2-426cf2a5b835",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "x = torch.randn(1)\n",
        "print(x)\n",
        "print(x.item())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([-0.1063])\n",
            "-0.10630612075328827\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XhsZ19XHIEmz",
        "colab_type": "text"
      },
      "source": [
        "**Read later:**\n",
        "\n",
        "\n",
        "  100+ Tensor operations, including transposing, indexing, slicing,\n",
        "  mathematical operations, linear algebra, random numbers, etc.,\n",
        "  are described\n",
        "  `here <http://pytorch.org/docs/torch>`_.\n",
        "\n",
        "NumPy Bridge\n",
        "------------\n",
        "\n",
        "Converting a Torch Tensor to a NumPy array and vice versa is a breeze.\n",
        "\n",
        "The Torch Tensor and NumPy array will share their underlying memory\n",
        "locations, and changing one will change the other.\n",
        "\n",
        "Converting a Torch Tensor to a NumPy Array"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLlflQ7xIEm0",
        "colab_type": "code",
        "outputId": "f34516b5-299f-45fd-cf9d-45c9ae939d07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "a = torch.ones(5)\n",
        "print(a)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([1., 1., 1., 1., 1.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5KM5UhXIEm2",
        "colab_type": "code",
        "outputId": "492a3980-e4d2-42c2-c8d3-cb4071a874b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "b = a.numpy()\n",
        "print(b)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1. 1. 1. 1. 1.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcjuW4ipIEm6",
        "colab_type": "text"
      },
      "source": [
        "See how the numpy array changed in value.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXeCGJSnIEm6",
        "colab_type": "code",
        "outputId": "b6d34b80-9e55-4bc4-ba3f-38d38c25d1fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "a.add_(1)\n",
        "print(a)\n",
        "print(b)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([2., 2., 2., 2., 2.])\n",
            "[2. 2. 2. 2. 2.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x25g4Jp3IEm8",
        "colab_type": "text"
      },
      "source": [
        "Converting NumPy Array to Torch Tensor\n",
        "--------------------------\n",
        "See how changing the np array changed the Torch Tensor automatically\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBmkcKFLIEm8",
        "colab_type": "code",
        "outputId": "95cd234c-a961-45c9-9289-aefe1c9909b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "import numpy as np\n",
        "a = np.ones(5)\n",
        "b = torch.from_numpy(a)\n",
        "np.add(a, 1, out=a)\n",
        "print(a)\n",
        "print(b)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2. 2. 2. 2. 2.]\n",
            "tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26STZIo0IEm-",
        "colab_type": "text"
      },
      "source": [
        "All the Tensors on the CPU except a CharTensor support converting to\n",
        "NumPy and back.\n",
        "\n",
        "CUDA Tensors\n",
        "------------\n",
        "\n",
        "Tensors can be moved onto any device using the ``.to`` method.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxDgMMAmIEm_",
        "colab_type": "code",
        "outputId": "aac94092-f705-4e82-9b84-359f7e13bd4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# let us run this cell only if CUDA is available\n",
        "# We will use ``torch.device`` objects to move tensors in and out of GPU\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")          # a CUDA device object\n",
        "    y = torch.ones_like(x, device=device)  # directly create a tensor on GPU\n",
        "    x = x.to(device)                       # or just use strings ``.to(\"cuda\")``\n",
        "    z = x + y\n",
        "    print(z)\n",
        "    print(z.to(\"cpu\", torch.double))       # ``.to`` can also change dtype together!"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0.8937], device='cuda:0')\n",
            "tensor([0.8937], dtype=torch.float64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJilxLa4LETk",
        "colab_type": "text"
      },
      "source": [
        "Automatic differentation\n",
        "=============\n",
        "\n",
        "Central to all neural networks in PyTorch is the ``autograd`` package. This let's us calculate the derivatives of parametric functions w.r.t all the parameters. This is used to perrform back-propagation to train neural nets. Let us see a few simple examples of how to use autograd."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gyiNdwywMes6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.autograd import Variable"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1ZuUh8wpeK5",
        "colab_type": "text"
      },
      "source": [
        "## **Simple backward**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ki-D_4SYk05K",
        "colab_type": "code",
        "outputId": "1ea97de6-0269-4d1d-841a-e2924d816e8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "x = Variable(torch.Tensor([2]), requires_grad=True)\n",
        "\n",
        "y = 4*x + 3\n",
        "\n",
        "y.backward()\n",
        "print(x.grad)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([4.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRL9dBAg-OEb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y.grad"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXJNtsZclC4x",
        "colab_type": "code",
        "outputId": "e196f43d-0530-412e-f7ad-c7b54a46f777",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "x1 = Variable(torch.Tensor([2]), requires_grad=True)\n",
        "x2 = Variable(torch.Tensor([3]), requires_grad=True)\n",
        "\n",
        "y = 4*x1 + 3*x2\n",
        "k = x1**3 + 4*x2**2\n",
        "\n",
        "y.backward()\n",
        "k.backward()\n",
        "\n",
        "print(x1.grad, x2.grad)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([16.]) tensor([27.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUFMTqFxqQ-8",
        "colab_type": "text"
      },
      "source": [
        "## torch.no_grad()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIOeTRusmaxL",
        "colab_type": "code",
        "outputId": "20a51532-da11-4964-ecba-613dd1bc9af9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        }
      },
      "source": [
        "x = Variable(torch.Tensor([2]), requires_grad=True)\n",
        "y = torch.Tensor([1])\n",
        "\n",
        "with torch.no_grad():\n",
        "  for _ in range(3):\n",
        "    y = y * x\n",
        "\n",
        "y.backward()\n",
        "print(x.grad)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-bbc30e91cea1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8V9lD19mqqBG",
        "colab_type": "code",
        "outputId": "b5c9d834-6694-45b6-c306-417d56227ff1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "x = Variable(torch.Tensor([2]), requires_grad=True)\n",
        "y = torch.Tensor([1])\n",
        "\n",
        "with torch.no_grad():\n",
        "  for _ in range(3):\n",
        "    y = y * x\n",
        "y = y * x\n",
        "\n",
        "y.backward()\n",
        "print(x.grad)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([8.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F63Ci3TISmJn",
        "colab_type": "text"
      },
      "source": [
        "# End-to-end ML pipeline example with MLP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NozPXG3nTM2C",
        "colab_type": "text"
      },
      "source": [
        "## Task\n",
        "\n",
        "We will first consider the task of regression. We will generate our own univariate data. Univariate data is easy to visualize."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4GArzgt1F6T",
        "colab_type": "text"
      },
      "source": [
        "## Data setup\n",
        "\n",
        "We will generate our own data. In particular, we will sample points on a polynomial curve. We will add random noise to the data points in ordeer to make the data realistic."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24dNDuTWSuOQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import math"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxde1nFbUZ_s",
        "colab_type": "code",
        "outputId": "e9347936-ab2a-4978-aaec-b31a343c053b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        }
      },
      "source": [
        "def f(x):\n",
        "  return (x - 0.2)*(x + 0.3)*(3*x - 1.5) + 10\n",
        "\n",
        "SIZE = 50\n",
        "x = np.linspace(-2, 2, SIZE, np.float32)\n",
        "y = f(x)\n",
        "\n",
        "# add noise\n",
        "y_noise = y + 2 * np.random.randn(SIZE)\n",
        "\n",
        "# data split into training and testing data\n",
        "idx = list(range(SIZE))\n",
        "np.random.shuffle(idx)\n",
        "\n",
        "x_train = x[idx][:40]\n",
        "y_train = y_noise[idx][:40]\n",
        "\n",
        "x_test = x[idx][40:]\n",
        "y_test = y_noise[idx][40:]\n",
        "\n",
        "# plot train and test points\n",
        "plt.plot(x_train, y_train, 'o')\n",
        "plt.plot(x_test, y_test, 'ro')\n",
        "plt.plot(x, y, 'k-')\n",
        "# TODO: add labels to the plot"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fbb646c0f60>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFxGJEUr4IiIx4v8DW0ui2v9CzMsAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQ6dMU7_OnT6",
        "colab_type": "text"
      },
      "source": [
        "PyTorch uses ``torch.tensor``, rather than numpy arrays, so we need to\n",
        "convert our data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMFrWE5VOnT6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train, y_train, x_test, y_test = map(\n",
        "    torch.FloatTensor, (x_train[:, np.newaxis], y_train[:, np.newaxis], x_test[:, np.newaxis], y_test[:, np.newaxis])\n",
        ")\n",
        "n, _ = x_train.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CX9Ow7eOcdRi",
        "colab_type": "code",
        "outputId": "6e28a0f5-f6e5-49e1-a817-ffb202362d7c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "x_train.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([40, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AiPht3Hpci4j",
        "colab_type": "code",
        "outputId": "3fa04bd5-5647-484d-be63-53dac67adc7c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "x_test.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVKV5Lr_UvII",
        "colab_type": "text"
      },
      "source": [
        "## Loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xMbpRMDUR92",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mse(inp, target):\n",
        "  return torch.mean((inp - target)**2)\n",
        "  \n",
        "loss_func = mse"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOdqF9drFdk0",
        "colab_type": "text"
      },
      "source": [
        "## Activation functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tb8q_ykLFhJU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def relu(x):\n",
        "  return torch.where(x > 0, x, torch.zeros(x.shape))\n",
        "\n",
        "def sigmoid(x):\n",
        "  return 1/(1 + torch.exp(-x))\n",
        "\n",
        "def tanh(x):\n",
        "  return (torch.exp(x) - torch.exp(-x))/(torch.exp(x) + torch.exp(-x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ox_TeIdHUshf",
        "colab_type": "text"
      },
      "source": [
        "## Model definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVwYBgGSTZVe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Mnist_mlp_classifier():\n",
        "  def __init__(self):\n",
        "    self.weights_1 = torch.randn(1, 800)\n",
        "    self.weights_1.requires_grad_()\n",
        "    self.bias_1 = torch.zeros(800, requires_grad=True)\n",
        "\n",
        "    self.weights_2 = torch.randn(800, 1)\n",
        "    self.weights_2.requires_grad_()\n",
        "    self.bias_2 = torch.zeros(1, requires_grad=True)\n",
        "  \n",
        "  def forward(self, xb):\n",
        "    h =  xb.matmul(self.weights_1) + self.bias_1\n",
        "    h = tanh(h)\n",
        "    h =  h.matmul(self.weights_2) + self.bias_2\n",
        "    out = h\n",
        "    return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knsjygzJU4YE",
        "colab_type": "text"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6IWQmfyUyLy",
        "colab_type": "code",
        "outputId": "0ba40ab7-a2e7-4f9a-c130-3ba8a3ea6b39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "lr = 0.0005  # learning rate\n",
        "epochs = 250  # how many epochs to train for\n",
        "bs = 10  # batch size\n",
        "\n",
        "model = Mnist_mlp_classifier()\n",
        "\n",
        "y_pred = model.forward(x_test)\n",
        "print('MSE of model after training: ', mse(y_pred, y_test))\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  for i in range((n - 1) // bs + 1):\n",
        "    start_i = i * bs\n",
        "    end_i = start_i + bs\n",
        "    xb = x_train[start_i:end_i]\n",
        "    yb = y_train[start_i:end_i]\n",
        "    pred = model.forward(xb)\n",
        "    loss = loss_func(pred, yb)\n",
        "    loss.backward()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "      model.weights_1 -= model.weights_1.grad * lr\n",
        "      model.bias_1 -= model.bias_1.grad * lr\n",
        "      model.weights_1.grad.zero_()\n",
        "      model.bias_1.grad.zero_()\n",
        "\n",
        "      model.weights_2 -= model.weights_2.grad * lr\n",
        "      model.bias_2 -= model.bias_2.grad * lr\n",
        "      model.weights_2.grad.zero_()\n",
        "      model.bias_2.grad.zero_()\n",
        "      \n",
        "  print('Loss at epoch %d : %f' % (epoch, loss))\n",
        "\n",
        "y_pred = model.forward(x_test)\n",
        "print('MSE of model after training: ', loss_func(y_pred, y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MSE of model after training:  tensor(120.3267, grad_fn=<MeanBackward0>)\n",
            "Loss at epoch 0 : 31.284595\n",
            "Loss at epoch 1 : 21.391603\n",
            "Loss at epoch 2 : 19.561281\n",
            "Loss at epoch 3 : 18.193947\n",
            "Loss at epoch 4 : 17.016077\n",
            "Loss at epoch 5 : 15.986241\n",
            "Loss at epoch 6 : 15.083712\n",
            "Loss at epoch 7 : 14.291759\n",
            "Loss at epoch 8 : 13.596022\n",
            "Loss at epoch 9 : 12.984096\n",
            "Loss at epoch 10 : 12.445265\n",
            "Loss at epoch 11 : 11.970261\n",
            "Loss at epoch 12 : 11.550993\n",
            "Loss at epoch 13 : 11.180456\n",
            "Loss at epoch 14 : 10.852633\n",
            "Loss at epoch 15 : 10.562223\n",
            "Loss at epoch 16 : 10.304646\n",
            "Loss at epoch 17 : 10.075865\n",
            "Loss at epoch 18 : 9.872429\n",
            "Loss at epoch 19 : 9.691301\n",
            "Loss at epoch 20 : 9.529798\n",
            "Loss at epoch 21 : 9.385622\n",
            "Loss at epoch 22 : 9.256731\n",
            "Loss at epoch 23 : 9.141359\n",
            "Loss at epoch 24 : 9.037945\n",
            "Loss at epoch 25 : 8.945091\n",
            "Loss at epoch 26 : 8.861648\n",
            "Loss at epoch 27 : 8.786531\n",
            "Loss at epoch 28 : 8.718807\n",
            "Loss at epoch 29 : 8.657660\n",
            "Loss at epoch 30 : 8.602373\n",
            "Loss at epoch 31 : 8.552328\n",
            "Loss at epoch 32 : 8.506948\n",
            "Loss at epoch 33 : 8.465723\n",
            "Loss at epoch 34 : 8.428235\n",
            "Loss at epoch 35 : 8.394098\n",
            "Loss at epoch 36 : 8.362939\n",
            "Loss at epoch 37 : 8.334478\n",
            "Loss at epoch 38 : 8.308434\n",
            "Loss at epoch 39 : 8.284595\n",
            "Loss at epoch 40 : 8.262670\n",
            "Loss at epoch 41 : 8.242565\n",
            "Loss at epoch 42 : 8.224024\n",
            "Loss at epoch 43 : 8.206964\n",
            "Loss at epoch 44 : 8.191192\n",
            "Loss at epoch 45 : 8.176611\n",
            "Loss at epoch 46 : 8.163126\n",
            "Loss at epoch 47 : 8.150619\n",
            "Loss at epoch 48 : 8.139002\n",
            "Loss at epoch 49 : 8.128196\n",
            "Loss at epoch 50 : 8.118143\n",
            "Loss at epoch 51 : 8.108778\n",
            "Loss at epoch 52 : 8.100042\n",
            "Loss at epoch 53 : 8.091853\n",
            "Loss at epoch 54 : 8.084208\n",
            "Loss at epoch 55 : 8.077039\n",
            "Loss at epoch 56 : 8.070302\n",
            "Loss at epoch 57 : 8.063988\n",
            "Loss at epoch 58 : 8.058050\n",
            "Loss at epoch 59 : 8.052461\n",
            "Loss at epoch 60 : 8.047199\n",
            "Loss at epoch 61 : 8.042213\n",
            "Loss at epoch 62 : 8.037521\n",
            "Loss at epoch 63 : 8.033076\n",
            "Loss at epoch 64 : 8.028876\n",
            "Loss at epoch 65 : 8.024909\n",
            "Loss at epoch 66 : 8.021135\n",
            "Loss at epoch 67 : 8.017529\n",
            "Loss at epoch 68 : 8.014135\n",
            "Loss at epoch 69 : 8.010901\n",
            "Loss at epoch 70 : 8.007801\n",
            "Loss at epoch 71 : 8.004889\n",
            "Loss at epoch 72 : 8.002096\n",
            "Loss at epoch 73 : 7.999418\n",
            "Loss at epoch 74 : 7.996873\n",
            "Loss at epoch 75 : 7.994438\n",
            "Loss at epoch 76 : 7.992110\n",
            "Loss at epoch 77 : 7.989884\n",
            "Loss at epoch 78 : 7.987737\n",
            "Loss at epoch 79 : 7.985701\n",
            "Loss at epoch 80 : 7.983744\n",
            "Loss at epoch 81 : 7.981888\n",
            "Loss at epoch 82 : 7.980068\n",
            "Loss at epoch 83 : 7.978342\n",
            "Loss at epoch 84 : 7.976689\n",
            "Loss at epoch 85 : 7.975089\n",
            "Loss at epoch 86 : 7.973553\n",
            "Loss at epoch 87 : 7.972076\n",
            "Loss at epoch 88 : 7.970658\n",
            "Loss at epoch 89 : 7.969290\n",
            "Loss at epoch 90 : 7.967969\n",
            "Loss at epoch 91 : 7.966710\n",
            "Loss at epoch 92 : 7.965478\n",
            "Loss at epoch 93 : 7.964314\n",
            "Loss at epoch 94 : 7.963171\n",
            "Loss at epoch 95 : 7.962087\n",
            "Loss at epoch 96 : 7.961028\n",
            "Loss at epoch 97 : 7.960001\n",
            "Loss at epoch 98 : 7.959025\n",
            "Loss at epoch 99 : 7.958064\n",
            "Loss at epoch 100 : 7.957143\n",
            "Loss at epoch 101 : 7.956268\n",
            "Loss at epoch 102 : 7.955414\n",
            "Loss at epoch 103 : 7.954585\n",
            "Loss at epoch 104 : 7.953796\n",
            "Loss at epoch 105 : 7.953040\n",
            "Loss at epoch 106 : 7.952295\n",
            "Loss at epoch 107 : 7.951579\n",
            "Loss at epoch 108 : 7.950883\n",
            "Loss at epoch 109 : 7.950218\n",
            "Loss at epoch 110 : 7.949571\n",
            "Loss at epoch 111 : 7.948966\n",
            "Loss at epoch 112 : 7.948359\n",
            "Loss at epoch 113 : 7.947798\n",
            "Loss at epoch 114 : 7.947238\n",
            "Loss at epoch 115 : 7.946710\n",
            "Loss at epoch 116 : 7.946185\n",
            "Loss at epoch 117 : 7.945674\n",
            "Loss at epoch 118 : 7.945208\n",
            "Loss at epoch 119 : 7.944749\n",
            "Loss at epoch 120 : 7.944305\n",
            "Loss at epoch 121 : 7.943876\n",
            "Loss at epoch 122 : 7.943461\n",
            "Loss at epoch 123 : 7.943067\n",
            "Loss at epoch 124 : 7.942694\n",
            "Loss at epoch 125 : 7.942328\n",
            "Loss at epoch 126 : 7.941991\n",
            "Loss at epoch 127 : 7.941655\n",
            "Loss at epoch 128 : 7.941332\n",
            "Loss at epoch 129 : 7.941036\n",
            "Loss at epoch 130 : 7.940741\n",
            "Loss at epoch 131 : 7.940469\n",
            "Loss at epoch 132 : 7.940205\n",
            "Loss at epoch 133 : 7.939960\n",
            "Loss at epoch 134 : 7.939721\n",
            "Loss at epoch 135 : 7.939493\n",
            "Loss at epoch 136 : 7.939279\n",
            "Loss at epoch 137 : 7.939070\n",
            "Loss at epoch 138 : 7.938904\n",
            "Loss at epoch 139 : 7.938707\n",
            "Loss at epoch 140 : 7.938551\n",
            "Loss at epoch 141 : 7.938399\n",
            "Loss at epoch 142 : 7.938248\n",
            "Loss at epoch 143 : 7.938115\n",
            "Loss at epoch 144 : 7.938005\n",
            "Loss at epoch 145 : 7.937860\n",
            "Loss at epoch 146 : 7.937770\n",
            "Loss at epoch 147 : 7.937658\n",
            "Loss at epoch 148 : 7.937596\n",
            "Loss at epoch 149 : 7.937505\n",
            "Loss at epoch 150 : 7.937432\n",
            "Loss at epoch 151 : 7.937389\n",
            "Loss at epoch 152 : 7.937328\n",
            "Loss at epoch 153 : 7.937296\n",
            "Loss at epoch 154 : 7.937269\n",
            "Loss at epoch 155 : 7.937243\n",
            "Loss at epoch 156 : 7.937229\n",
            "Loss at epoch 157 : 7.937232\n",
            "Loss at epoch 158 : 7.937219\n",
            "Loss at epoch 159 : 7.937223\n",
            "Loss at epoch 160 : 7.937246\n",
            "Loss at epoch 161 : 7.937270\n",
            "Loss at epoch 162 : 7.937303\n",
            "Loss at epoch 163 : 7.937337\n",
            "Loss at epoch 164 : 7.937396\n",
            "Loss at epoch 165 : 7.937443\n",
            "Loss at epoch 166 : 7.937502\n",
            "Loss at epoch 167 : 7.937566\n",
            "Loss at epoch 168 : 7.937630\n",
            "Loss at epoch 169 : 7.937717\n",
            "Loss at epoch 170 : 7.937808\n",
            "Loss at epoch 171 : 7.937887\n",
            "Loss at epoch 172 : 7.938000\n",
            "Loss at epoch 173 : 7.938082\n",
            "Loss at epoch 174 : 7.938199\n",
            "Loss at epoch 175 : 7.938333\n",
            "Loss at epoch 176 : 7.938445\n",
            "Loss at epoch 177 : 7.938579\n",
            "Loss at epoch 178 : 7.938704\n",
            "Loss at epoch 179 : 7.938851\n",
            "Loss at epoch 180 : 7.938997\n",
            "Loss at epoch 181 : 7.939134\n",
            "Loss at epoch 182 : 7.939296\n",
            "Loss at epoch 183 : 7.939458\n",
            "Loss at epoch 184 : 7.939614\n",
            "Loss at epoch 185 : 7.939795\n",
            "Loss at epoch 186 : 7.939960\n",
            "Loss at epoch 187 : 7.940146\n",
            "Loss at epoch 188 : 7.940322\n",
            "Loss at epoch 189 : 7.940513\n",
            "Loss at epoch 190 : 7.940705\n",
            "Loss at epoch 191 : 7.940898\n",
            "Loss at epoch 192 : 7.941111\n",
            "Loss at epoch 193 : 7.941318\n",
            "Loss at epoch 194 : 7.941518\n",
            "Loss at epoch 195 : 7.941725\n",
            "Loss at epoch 196 : 7.941944\n",
            "Loss at epoch 197 : 7.942172\n",
            "Loss at epoch 198 : 7.942402\n",
            "Loss at epoch 199 : 7.942629\n",
            "Loss at epoch 200 : 7.942858\n",
            "Loss at epoch 201 : 7.943096\n",
            "Loss at epoch 202 : 7.943329\n",
            "Loss at epoch 203 : 7.943573\n",
            "Loss at epoch 204 : 7.943824\n",
            "Loss at epoch 205 : 7.944067\n",
            "Loss at epoch 206 : 7.944327\n",
            "Loss at epoch 207 : 7.944582\n",
            "Loss at epoch 208 : 7.944857\n",
            "Loss at epoch 209 : 7.945107\n",
            "Loss at epoch 210 : 7.945384\n",
            "Loss at epoch 211 : 7.945652\n",
            "Loss at epoch 212 : 7.945903\n",
            "Loss at epoch 213 : 7.946191\n",
            "Loss at epoch 214 : 7.946465\n",
            "Loss at epoch 215 : 7.946755\n",
            "Loss at epoch 216 : 7.947034\n",
            "Loss at epoch 217 : 7.947315\n",
            "Loss at epoch 218 : 7.947594\n",
            "Loss at epoch 219 : 7.947898\n",
            "Loss at epoch 220 : 7.948176\n",
            "Loss at epoch 221 : 7.948471\n",
            "Loss at epoch 222 : 7.948790\n",
            "Loss at epoch 223 : 7.949080\n",
            "Loss at epoch 224 : 7.949388\n",
            "Loss at epoch 225 : 7.949698\n",
            "Loss at epoch 226 : 7.949982\n",
            "Loss at epoch 227 : 7.950313\n",
            "Loss at epoch 228 : 7.950600\n",
            "Loss at epoch 229 : 7.950926\n",
            "Loss at epoch 230 : 7.951234\n",
            "Loss at epoch 231 : 7.951552\n",
            "Loss at epoch 232 : 7.951865\n",
            "Loss at epoch 233 : 7.952193\n",
            "Loss at epoch 234 : 7.952510\n",
            "Loss at epoch 235 : 7.952842\n",
            "Loss at epoch 236 : 7.953168\n",
            "Loss at epoch 237 : 7.953501\n",
            "Loss at epoch 238 : 7.953818\n",
            "Loss at epoch 239 : 7.954151\n",
            "Loss at epoch 240 : 7.954488\n",
            "Loss at epoch 241 : 7.954811\n",
            "Loss at epoch 242 : 7.955152\n",
            "Loss at epoch 243 : 7.955482\n",
            "Loss at epoch 244 : 7.955819\n",
            "Loss at epoch 245 : 7.956182\n",
            "Loss at epoch 246 : 7.956511\n",
            "Loss at epoch 247 : 7.956851\n",
            "Loss at epoch 248 : 7.957198\n",
            "Loss at epoch 249 : 7.957528\n",
            "MSE of model after training:  tensor(4.2957, grad_fn=<MeanBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXNWCAEIPgw4",
        "colab_type": "text"
      },
      "source": [
        "## Visualizing the trained model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "bd5afbb8-1cf5-4971-aced-76a98d095fd7",
        "id": "ajnc5IT_HcFo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        }
      },
      "source": [
        "x = torch.linspace(-2, 2, 50).unsqueeze(1)\n",
        "y_ = model.forward(x)\n",
        "y = f(x)\n",
        "plt.plot(x_train.numpy(), y_train.detach().numpy(), 'o')\n",
        "plt.plot(x.numpy(), y_.detach().numpy(), 'b-')\n",
        "plt.plot(x.numpy(), y.detach().numpy(), 'k-')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fbb61df75f8>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n4P4m09QYXAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbvpkMb2Hp8g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}