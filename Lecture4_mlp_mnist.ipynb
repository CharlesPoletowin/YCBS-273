{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lecture4_mlp_mnist.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CharlesPoletowin/YCBS-273/blob/master/Lecture4_mlp_mnist.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATgPnJ-cI8n-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "from pathlib import Path\n",
        "import requests\n",
        "import gzip\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Gkl2XEYK-HU",
        "colab_type": "text"
      },
      "source": [
        "# End-to-end ML pipeline example with MNIST dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EF6BzJMzLTvj",
        "colab_type": "text"
      },
      "source": [
        "## MNIST data setup\n",
        "\n",
        "MNIST dataset has a training set of 60,000 examples, and a test set of 10,000 examples. Each image is of size 28 x 28.\n",
        "\n",
        "Original source: http://yann.lecun.com/exdb/mnist/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxP5PWQGK-u7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATA_PATH = Path(\"data\")\n",
        "PATH = DATA_PATH / \"mnist\"\n",
        "\n",
        "PATH.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def get_data(url, file_name, offset):\n",
        "  content = requests.get(url).content\n",
        "  (PATH / file_name).open(\"wb\").write(content)\n",
        "  with gzip.open((PATH / file_name).as_posix(), \"rb\") as f:\n",
        "    data = np.frombuffer(f.read(), dtype=np.uint8, offset=offset)\n",
        "  return data\n",
        "\n",
        "y_train = get_data(\"http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\", \"train-labels\", 8).astype('int64')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNf_a3KYBe3E",
        "colab_type": "code",
        "outputId": "d2401e9a-1abc-49a0-df55-4bfb24b3b828",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "y_train.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UieRFAoWBXqY",
        "colab_type": "code",
        "outputId": "a7dc550b-0272-496a-c447-be99b41ec3c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "x_train = get_data(\"http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\", \"train-images\", 16).reshape(len(y_train), 784).astype('float32')\n",
        "x_train.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 784)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iib8BMoZBafv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_test = get_data(\"http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\", \"test-labels\", 8).astype('int64')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6m2zXOqABd4q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_test = get_data(\"http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\", \"test-images\", 16).reshape(len(y_test), 784).astype('float32')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsZ5zwGYBVjE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train, y_train, x_test, y_test = map(\n",
        "    torch.tensor, (x_train, y_train, x_test, y_test)\n",
        ")\n",
        "n = x_train.shape[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdI5O53mNsKd",
        "colab_type": "text"
      },
      "source": [
        "## Model definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5k3IxIZCMRwb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Mnist_mlp_classifier(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.layer1 = nn.Linear(784, 200)    \n",
        "    self.layer2 = nn.Linear(200, 10)\n",
        "\n",
        "  def forward(self, xb):\n",
        "    xb = F.relu(self.layer1(xb))\n",
        "    xb = F.relu(self.layer2(xb))\n",
        "    return xb.view(-1, xb.size(1))\n",
        "  \n",
        "class Mnist_cnn_classifier(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv2d(in_channels=1, out_channels=4, kernel_size=(2, 2), stride=2, padding=0)\n",
        "    self.pool1 = nn.MaxPool2d(kernel_size=(2, 2), padding=0)\n",
        "    \n",
        "    self.conv2 = nn.Conv2d(in_channels=4, out_channels=8, kernel_size=(2, 2), stride=1, padding=0)\n",
        "    self.pool2 = nn.MaxPool2d(kernel_size=(2, 2), padding=0)\n",
        "    \n",
        "    self.fc_layer = nn.Linear(72, 10)\n",
        "\n",
        "  def forward(self, xb):\n",
        "    h1 = self.conv1(xb)\n",
        "    h1 = self.pool1(h1)\n",
        "    h1 = F.relu(h1)\n",
        "    \n",
        "    h2 = self.conv2(h1)\n",
        "    h2 = self.pool2(h2)\n",
        "    h2 = F.relu(h2)  # 8 x 3 x 3\n",
        "    \n",
        "    # flatten the output from conv layers before feeind it to FC layer\n",
        "    h2 = h2.view(-1, 72)\n",
        "    out = self.fc_layer(h2)\n",
        "    \n",
        "    return out\n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eP8nd9EhN4-I",
        "colab_type": "text"
      },
      "source": [
        "## Training loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FHAHkN4NowD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(model, x_train, y_train, x_test, y_test, epochs=250, batch_size=64, lr=0.01, weight_decay=0):\n",
        "  # data\n",
        "  train_dataset = TensorDataset(x_train, y_train)\n",
        "  train_data_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
        "  \n",
        "  # loss function\n",
        "  loss_func = F.cross_entropy\n",
        "\n",
        "  # optimizer\n",
        "  optimizer = optim.SGD(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "  # training loop\n",
        "  for epoch in range(epochs):\n",
        "    for xb, yb in train_data_loader:\n",
        "      \n",
        "      pred = model(xb)\n",
        "      loss = loss_func(pred, yb)\n",
        "\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "    y_pred = model(x_test)\n",
        "    acc = accuracy_score(torch.argmax(y_pred, dim=1).detach().numpy(), y_test)\n",
        "    \n",
        "    print('Loss at epoch %d : %f, test acc: %f' % (epoch, loss, acc))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ru425tEqOkUs",
        "colab_type": "code",
        "outputId": "0d8dc4e6-375a-4823-9dab-a6d42d6d1efa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 921
        }
      },
      "source": [
        "mlp_model = Mnist_mlp_classifier()\n",
        "train_model(mlp_model, x_train, y_train, x_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss at epoch 0 : 1.123346, test acc: 0.678600\n",
            "Loss at epoch 1 : 1.075555, test acc: 0.637400\n",
            "Loss at epoch 2 : 0.911502, test acc: 0.726000\n",
            "Loss at epoch 3 : 0.729620, test acc: 0.734200\n",
            "Loss at epoch 4 : 0.610144, test acc: 0.742400\n",
            "Loss at epoch 5 : 0.689837, test acc: 0.731700\n",
            "Loss at epoch 6 : 0.598341, test acc: 0.614200\n",
            "Loss at epoch 7 : 0.853705, test acc: 0.636700\n",
            "Loss at epoch 8 : 1.201001, test acc: 0.557400\n",
            "Loss at epoch 9 : 0.905252, test acc: 0.635600\n",
            "Loss at epoch 10 : 0.900863, test acc: 0.645800\n",
            "Loss at epoch 11 : 1.185689, test acc: 0.528100\n",
            "Loss at epoch 12 : 1.181128, test acc: 0.566600\n",
            "Loss at epoch 13 : 0.969500, test acc: 0.641200\n",
            "Loss at epoch 14 : 0.962378, test acc: 0.641100\n",
            "Loss at epoch 15 : 0.961007, test acc: 0.647900\n",
            "Loss at epoch 16 : 1.019719, test acc: 0.593000\n",
            "Loss at epoch 17 : 1.174369, test acc: 0.559700\n",
            "Loss at epoch 18 : 0.813903, test acc: 0.631900\n",
            "Loss at epoch 19 : 0.889370, test acc: 0.630100\n",
            "Loss at epoch 20 : 0.820430, test acc: 0.641600\n",
            "Loss at epoch 21 : 1.027880, test acc: 0.653200\n",
            "Loss at epoch 22 : 1.093530, test acc: 0.634100\n",
            "Loss at epoch 23 : 0.739364, test acc: 0.728100\n",
            "Loss at epoch 24 : 0.677543, test acc: 0.686400\n",
            "Loss at epoch 25 : 0.740220, test acc: 0.741800\n",
            "Loss at epoch 26 : 0.955794, test acc: 0.706700\n",
            "Loss at epoch 27 : 0.674956, test acc: 0.749500\n",
            "Loss at epoch 28 : 0.597198, test acc: 0.745300\n",
            "Loss at epoch 29 : 0.761846, test acc: 0.742000\n",
            "Loss at epoch 30 : 0.596803, test acc: 0.748100\n",
            "Loss at epoch 31 : 0.596187, test acc: 0.748800\n",
            "Loss at epoch 32 : 0.596257, test acc: 0.750000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-2c2acc1ab585>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmlp_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMnist_mlp_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmlp_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-10-5caeee302aa1>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, x_train, y_train, x_test, y_test, epochs, batch_size, lr, weight_decay)\u001b[0m\n\u001b[1;32m     17\u001b[0m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m       \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mgrad_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m     \u001b[0mgrad_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mretain_graph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads)\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mnew_grads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mgrad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"grad can be implicitly created only for scalar outputs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-cBS2HxJ9eWA",
        "colab_type": "code",
        "outputId": "9425f0fa-3521-40c1-a610-94149035678d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "cnn_model = Mnist_cnn_classifier()\n",
        "train_model(cnn_model, x_train.view(-1, 1, 28, 28), y_train, x_test.view(-1, 1, 28, 28), y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss at epoch 0 : 2.307692, test acc: 0.102800\n",
            "Loss at epoch 1 : 2.306761, test acc: 0.113500\n",
            "Loss at epoch 2 : 2.306543, test acc: 0.113500\n",
            "Loss at epoch 3 : 2.306474, test acc: 0.113500\n",
            "Loss at epoch 4 : 2.306448, test acc: 0.113500\n",
            "Loss at epoch 5 : 2.306437, test acc: 0.113500\n",
            "Loss at epoch 6 : 2.306433, test acc: 0.113500\n",
            "Loss at epoch 7 : 2.306432, test acc: 0.113500\n",
            "Loss at epoch 8 : 2.306432, test acc: 0.113500\n",
            "Loss at epoch 9 : 2.306430, test acc: 0.113500\n",
            "Loss at epoch 10 : 2.306430, test acc: 0.113500\n",
            "Loss at epoch 11 : 2.306430, test acc: 0.113500\n",
            "Loss at epoch 12 : 2.306431, test acc: 0.113500\n",
            "Loss at epoch 13 : 2.306431, test acc: 0.113500\n",
            "Loss at epoch 14 : 2.306431, test acc: 0.113500\n",
            "Loss at epoch 15 : 2.306431, test acc: 0.113500\n",
            "Loss at epoch 16 : 2.306431, test acc: 0.113500\n",
            "Loss at epoch 17 : 2.306431, test acc: 0.113500\n",
            "Loss at epoch 18 : 2.306431, test acc: 0.113500\n",
            "Loss at epoch 19 : 2.306431, test acc: 0.113500\n",
            "Loss at epoch 20 : 2.306431, test acc: 0.113500\n",
            "Loss at epoch 21 : 2.306431, test acc: 0.113500\n",
            "Loss at epoch 22 : 2.306431, test acc: 0.113500\n",
            "Loss at epoch 23 : 2.306431, test acc: 0.113500\n",
            "Loss at epoch 24 : 2.306431, test acc: 0.113500\n",
            "Loss at epoch 25 : 2.306431, test acc: 0.113500\n",
            "Loss at epoch 26 : 2.306431, test acc: 0.113500\n",
            "Loss at epoch 27 : 2.306431, test acc: 0.113500\n",
            "Loss at epoch 28 : 2.306431, test acc: 0.113500\n",
            "Loss at epoch 29 : 2.306431, test acc: 0.113500\n",
            "Loss at epoch 30 : 2.306431, test acc: 0.113500\n",
            "Loss at epoch 31 : 2.306431, test acc: 0.113500\n",
            "Loss at epoch 32 : 2.306431, test acc: 0.113500\n",
            "Loss at epoch 33 : 2.306431, test acc: 0.113500\n",
            "Loss at epoch 34 : 2.306431, test acc: 0.113500\n",
            "Loss at epoch 35 : 2.306431, test acc: 0.113500\n",
            "Loss at epoch 36 : 2.306431, test acc: 0.113500\n",
            "Loss at epoch 37 : 2.306431, test acc: 0.113500\n",
            "Loss at epoch 38 : 2.306431, test acc: 0.113500\n",
            "Loss at epoch 39 : 2.306431, test acc: 0.113500\n",
            "Loss at epoch 40 : 2.306430, test acc: 0.113500\n",
            "Loss at epoch 41 : 2.306430, test acc: 0.113500\n",
            "Loss at epoch 42 : 2.306430, test acc: 0.113500\n",
            "Loss at epoch 43 : 2.306430, test acc: 0.113500\n",
            "Loss at epoch 44 : 2.306430, test acc: 0.113500\n",
            "Loss at epoch 45 : 2.306430, test acc: 0.113500\n",
            "Loss at epoch 46 : 2.306430, test acc: 0.113500\n",
            "Loss at epoch 47 : 2.306429, test acc: 0.113500\n",
            "Loss at epoch 48 : 2.306429, test acc: 0.113500\n",
            "Loss at epoch 49 : 2.306429, test acc: 0.113500\n",
            "Loss at epoch 50 : 2.306429, test acc: 0.113500\n",
            "Loss at epoch 51 : 2.306429, test acc: 0.113500\n",
            "Loss at epoch 52 : 2.306429, test acc: 0.113500\n",
            "Loss at epoch 53 : 2.306429, test acc: 0.113500\n",
            "Loss at epoch 54 : 2.306428, test acc: 0.113500\n",
            "Loss at epoch 55 : 2.306429, test acc: 0.113500\n",
            "Loss at epoch 56 : 2.306429, test acc: 0.113500\n",
            "Loss at epoch 57 : 2.306428, test acc: 0.113500\n",
            "Loss at epoch 58 : 2.306428, test acc: 0.113500\n",
            "Loss at epoch 59 : 2.306428, test acc: 0.113500\n",
            "Loss at epoch 60 : 2.306427, test acc: 0.113500\n",
            "Loss at epoch 61 : 2.306428, test acc: 0.113500\n",
            "Loss at epoch 62 : 2.306428, test acc: 0.113500\n",
            "Loss at epoch 63 : 2.306428, test acc: 0.113500\n",
            "Loss at epoch 64 : 2.306427, test acc: 0.113500\n",
            "Loss at epoch 65 : 2.306427, test acc: 0.113500\n",
            "Loss at epoch 66 : 2.306427, test acc: 0.113500\n",
            "Loss at epoch 67 : 2.306427, test acc: 0.113500\n",
            "Loss at epoch 68 : 2.306426, test acc: 0.113500\n",
            "Loss at epoch 69 : 2.306426, test acc: 0.113500\n",
            "Loss at epoch 70 : 2.306426, test acc: 0.113500\n",
            "Loss at epoch 71 : 2.306426, test acc: 0.113500\n",
            "Loss at epoch 72 : 2.306426, test acc: 0.113500\n",
            "Loss at epoch 73 : 2.306425, test acc: 0.113500\n",
            "Loss at epoch 74 : 2.306425, test acc: 0.113500\n",
            "Loss at epoch 75 : 2.306425, test acc: 0.113500\n",
            "Loss at epoch 76 : 2.306425, test acc: 0.113500\n",
            "Loss at epoch 77 : 2.306425, test acc: 0.113500\n",
            "Loss at epoch 78 : 2.306424, test acc: 0.113500\n",
            "Loss at epoch 79 : 2.306424, test acc: 0.113500\n",
            "Loss at epoch 80 : 2.306424, test acc: 0.113500\n",
            "Loss at epoch 81 : 2.306424, test acc: 0.113500\n",
            "Loss at epoch 82 : 2.306423, test acc: 0.113500\n",
            "Loss at epoch 83 : 2.306423, test acc: 0.113500\n",
            "Loss at epoch 84 : 2.306423, test acc: 0.113500\n",
            "Loss at epoch 85 : 2.306422, test acc: 0.113500\n",
            "Loss at epoch 86 : 2.306422, test acc: 0.113500\n",
            "Loss at epoch 87 : 2.306422, test acc: 0.113500\n",
            "Loss at epoch 88 : 2.306421, test acc: 0.113500\n",
            "Loss at epoch 89 : 2.306421, test acc: 0.113500\n",
            "Loss at epoch 90 : 2.306421, test acc: 0.113500\n",
            "Loss at epoch 91 : 2.306421, test acc: 0.113500\n",
            "Loss at epoch 92 : 2.306420, test acc: 0.113500\n",
            "Loss at epoch 93 : 2.306420, test acc: 0.113500\n",
            "Loss at epoch 94 : 2.306420, test acc: 0.113500\n",
            "Loss at epoch 95 : 2.306419, test acc: 0.113500\n",
            "Loss at epoch 96 : 2.306419, test acc: 0.113500\n",
            "Loss at epoch 97 : 2.306418, test acc: 0.113500\n",
            "Loss at epoch 98 : 2.306418, test acc: 0.113500\n",
            "Loss at epoch 99 : 2.306418, test acc: 0.113500\n",
            "Loss at epoch 100 : 2.306418, test acc: 0.113500\n",
            "Loss at epoch 101 : 2.306417, test acc: 0.113500\n",
            "Loss at epoch 102 : 2.306417, test acc: 0.113500\n",
            "Loss at epoch 103 : 2.306417, test acc: 0.113500\n",
            "Loss at epoch 104 : 2.306416, test acc: 0.113500\n",
            "Loss at epoch 105 : 2.306416, test acc: 0.113500\n",
            "Loss at epoch 106 : 2.306416, test acc: 0.113500\n",
            "Loss at epoch 107 : 2.306415, test acc: 0.113500\n",
            "Loss at epoch 108 : 2.306414, test acc: 0.113500\n",
            "Loss at epoch 109 : 2.306414, test acc: 0.113500\n",
            "Loss at epoch 110 : 2.306414, test acc: 0.113500\n",
            "Loss at epoch 111 : 2.306413, test acc: 0.113500\n",
            "Loss at epoch 112 : 2.306413, test acc: 0.113500\n",
            "Loss at epoch 113 : 2.306412, test acc: 0.113500\n",
            "Loss at epoch 114 : 2.306412, test acc: 0.113500\n",
            "Loss at epoch 115 : 2.306412, test acc: 0.113500\n",
            "Loss at epoch 116 : 2.306412, test acc: 0.113500\n",
            "Loss at epoch 117 : 2.306411, test acc: 0.113500\n",
            "Loss at epoch 118 : 2.306411, test acc: 0.113500\n",
            "Loss at epoch 119 : 2.306410, test acc: 0.113500\n",
            "Loss at epoch 120 : 2.306410, test acc: 0.113500\n",
            "Loss at epoch 121 : 2.306410, test acc: 0.113500\n",
            "Loss at epoch 122 : 2.306409, test acc: 0.113500\n",
            "Loss at epoch 123 : 2.306409, test acc: 0.113500\n",
            "Loss at epoch 124 : 2.306408, test acc: 0.113500\n",
            "Loss at epoch 125 : 2.306407, test acc: 0.113500\n",
            "Loss at epoch 126 : 2.306407, test acc: 0.113500\n",
            "Loss at epoch 127 : 2.306406, test acc: 0.113500\n",
            "Loss at epoch 128 : 2.306406, test acc: 0.113500\n",
            "Loss at epoch 129 : 2.306405, test acc: 0.113500\n",
            "Loss at epoch 130 : 2.306405, test acc: 0.113500\n",
            "Loss at epoch 131 : 2.306405, test acc: 0.113500\n",
            "Loss at epoch 132 : 2.306404, test acc: 0.113500\n",
            "Loss at epoch 133 : 2.306403, test acc: 0.113500\n",
            "Loss at epoch 134 : 2.306403, test acc: 0.113500\n",
            "Loss at epoch 135 : 2.306403, test acc: 0.113500\n",
            "Loss at epoch 136 : 2.306402, test acc: 0.113500\n",
            "Loss at epoch 137 : 2.306401, test acc: 0.113500\n",
            "Loss at epoch 138 : 2.306401, test acc: 0.113500\n",
            "Loss at epoch 139 : 2.306400, test acc: 0.113500\n",
            "Loss at epoch 140 : 2.306400, test acc: 0.113500\n",
            "Loss at epoch 141 : 2.306399, test acc: 0.113500\n",
            "Loss at epoch 142 : 2.306399, test acc: 0.113500\n",
            "Loss at epoch 143 : 2.306398, test acc: 0.113500\n",
            "Loss at epoch 144 : 2.306398, test acc: 0.113500\n",
            "Loss at epoch 145 : 2.306397, test acc: 0.113500\n",
            "Loss at epoch 146 : 2.306397, test acc: 0.113500\n",
            "Loss at epoch 147 : 2.306396, test acc: 0.113500\n",
            "Loss at epoch 148 : 2.306395, test acc: 0.113500\n",
            "Loss at epoch 149 : 2.306395, test acc: 0.113500\n",
            "Loss at epoch 150 : 2.306394, test acc: 0.113500\n",
            "Loss at epoch 151 : 2.306394, test acc: 0.113500\n",
            "Loss at epoch 152 : 2.306393, test acc: 0.113500\n",
            "Loss at epoch 153 : 2.306392, test acc: 0.113500\n",
            "Loss at epoch 154 : 2.306392, test acc: 0.113500\n",
            "Loss at epoch 155 : 2.306391, test acc: 0.113500\n",
            "Loss at epoch 156 : 2.306390, test acc: 0.113500\n",
            "Loss at epoch 157 : 2.306390, test acc: 0.113500\n",
            "Loss at epoch 158 : 2.306389, test acc: 0.113500\n",
            "Loss at epoch 159 : 2.306389, test acc: 0.113500\n",
            "Loss at epoch 160 : 2.306388, test acc: 0.113500\n",
            "Loss at epoch 161 : 2.306387, test acc: 0.113500\n",
            "Loss at epoch 162 : 2.306387, test acc: 0.113500\n",
            "Loss at epoch 163 : 2.306386, test acc: 0.113500\n",
            "Loss at epoch 164 : 2.306385, test acc: 0.113500\n",
            "Loss at epoch 165 : 2.306385, test acc: 0.113500\n",
            "Loss at epoch 166 : 2.306384, test acc: 0.113500\n",
            "Loss at epoch 167 : 2.306384, test acc: 0.113500\n",
            "Loss at epoch 168 : 2.306383, test acc: 0.113500\n",
            "Loss at epoch 169 : 2.306382, test acc: 0.113500\n",
            "Loss at epoch 170 : 2.306382, test acc: 0.113500\n",
            "Loss at epoch 171 : 2.306381, test acc: 0.113500\n",
            "Loss at epoch 172 : 2.306381, test acc: 0.113500\n",
            "Loss at epoch 173 : 2.306380, test acc: 0.113500\n",
            "Loss at epoch 174 : 2.306379, test acc: 0.113500\n",
            "Loss at epoch 175 : 2.306378, test acc: 0.113500\n",
            "Loss at epoch 176 : 2.306377, test acc: 0.113500\n",
            "Loss at epoch 177 : 2.306377, test acc: 0.113500\n",
            "Loss at epoch 178 : 2.306376, test acc: 0.113500\n",
            "Loss at epoch 179 : 2.306375, test acc: 0.113500\n",
            "Loss at epoch 180 : 2.306375, test acc: 0.113500\n",
            "Loss at epoch 181 : 2.306374, test acc: 0.113500\n",
            "Loss at epoch 182 : 2.306373, test acc: 0.113500\n",
            "Loss at epoch 183 : 2.306373, test acc: 0.113500\n",
            "Loss at epoch 184 : 2.306372, test acc: 0.113500\n",
            "Loss at epoch 185 : 2.306371, test acc: 0.113500\n",
            "Loss at epoch 186 : 2.306370, test acc: 0.113500\n",
            "Loss at epoch 187 : 2.306369, test acc: 0.113500\n",
            "Loss at epoch 188 : 2.306368, test acc: 0.113500\n",
            "Loss at epoch 189 : 2.306368, test acc: 0.113500\n",
            "Loss at epoch 190 : 2.306367, test acc: 0.113500\n",
            "Loss at epoch 191 : 2.306366, test acc: 0.113500\n",
            "Loss at epoch 192 : 2.306366, test acc: 0.113500\n",
            "Loss at epoch 193 : 2.306365, test acc: 0.113500\n",
            "Loss at epoch 194 : 2.306364, test acc: 0.113500\n",
            "Loss at epoch 195 : 2.306363, test acc: 0.113500\n",
            "Loss at epoch 196 : 2.306363, test acc: 0.113500\n",
            "Loss at epoch 197 : 2.306362, test acc: 0.113500\n",
            "Loss at epoch 198 : 2.306361, test acc: 0.113500\n",
            "Loss at epoch 199 : 2.306360, test acc: 0.113500\n",
            "Loss at epoch 200 : 2.306360, test acc: 0.113500\n",
            "Loss at epoch 201 : 2.306358, test acc: 0.113500\n",
            "Loss at epoch 202 : 2.306358, test acc: 0.113500\n",
            "Loss at epoch 203 : 2.306356, test acc: 0.113500\n",
            "Loss at epoch 204 : 2.306355, test acc: 0.113500\n",
            "Loss at epoch 205 : 2.306355, test acc: 0.113500\n",
            "Loss at epoch 206 : 2.306354, test acc: 0.113500\n",
            "Loss at epoch 207 : 2.306353, test acc: 0.113500\n",
            "Loss at epoch 208 : 2.306353, test acc: 0.113500\n",
            "Loss at epoch 209 : 2.306351, test acc: 0.113500\n",
            "Loss at epoch 210 : 2.306351, test acc: 0.113500\n",
            "Loss at epoch 211 : 2.306350, test acc: 0.113500\n",
            "Loss at epoch 212 : 2.306349, test acc: 0.113500\n",
            "Loss at epoch 213 : 2.306348, test acc: 0.113500\n",
            "Loss at epoch 214 : 2.306347, test acc: 0.113500\n",
            "Loss at epoch 215 : 2.306346, test acc: 0.113500\n",
            "Loss at epoch 216 : 2.306346, test acc: 0.113500\n",
            "Loss at epoch 217 : 2.306345, test acc: 0.113500\n",
            "Loss at epoch 218 : 2.306344, test acc: 0.113500\n",
            "Loss at epoch 219 : 2.306343, test acc: 0.113500\n",
            "Loss at epoch 220 : 2.306341, test acc: 0.113500\n",
            "Loss at epoch 221 : 2.306341, test acc: 0.113500\n",
            "Loss at epoch 222 : 2.306340, test acc: 0.113500\n",
            "Loss at epoch 223 : 2.306339, test acc: 0.113500\n",
            "Loss at epoch 224 : 2.306338, test acc: 0.113500\n",
            "Loss at epoch 225 : 2.306337, test acc: 0.113500\n",
            "Loss at epoch 226 : 2.306336, test acc: 0.113500\n",
            "Loss at epoch 227 : 2.306335, test acc: 0.113500\n",
            "Loss at epoch 228 : 2.306334, test acc: 0.113500\n",
            "Loss at epoch 229 : 2.306334, test acc: 0.113500\n",
            "Loss at epoch 230 : 2.306332, test acc: 0.113500\n",
            "Loss at epoch 231 : 2.306331, test acc: 0.113500\n",
            "Loss at epoch 232 : 2.306330, test acc: 0.113500\n",
            "Loss at epoch 233 : 2.306329, test acc: 0.113500\n",
            "Loss at epoch 234 : 2.306329, test acc: 0.113500\n",
            "Loss at epoch 235 : 2.306327, test acc: 0.113500\n",
            "Loss at epoch 236 : 2.306326, test acc: 0.113500\n",
            "Loss at epoch 237 : 2.306326, test acc: 0.113500\n",
            "Loss at epoch 238 : 2.306324, test acc: 0.113500\n",
            "Loss at epoch 239 : 2.306323, test acc: 0.113500\n",
            "Loss at epoch 240 : 2.306323, test acc: 0.113500\n",
            "Loss at epoch 241 : 2.306321, test acc: 0.113500\n",
            "Loss at epoch 242 : 2.306321, test acc: 0.113500\n",
            "Loss at epoch 243 : 2.306320, test acc: 0.113500\n",
            "Loss at epoch 244 : 2.306319, test acc: 0.113500\n",
            "Loss at epoch 245 : 2.306317, test acc: 0.113500\n",
            "Loss at epoch 246 : 2.306316, test acc: 0.113500\n",
            "Loss at epoch 247 : 2.306315, test acc: 0.113500\n",
            "Loss at epoch 248 : 2.306315, test acc: 0.113500\n",
            "Loss at epoch 249 : 2.306314, test acc: 0.113500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfPWPBAqOzCC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}