{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "Lecture2_mlp.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "QI6mmsdkLGFC",
        "6uTwW_1MPOpc"
      ],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CharlesPoletowin/YCBS-273/blob/master/Lecture2_mlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nE33IvZIEl9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FxqGMk-IEmC",
        "colab_type": "text"
      },
      "source": [
        "Disclaimer: This notebook is modified from the following notebooks:\n",
        "- https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/_downloads/nn_tutorial.ipynb#scrollTo=N29T2vcqOnVU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F63Ci3TISmJn",
        "colab_type": "text"
      },
      "source": [
        "# End-to-end ML pipeline example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NozPXG3nTM2C",
        "colab_type": "text"
      },
      "source": [
        "## Data setup\n",
        "\n",
        "We will generate our own data. In particular, we will sample points on a polynomial curve. We will add random noise to the data points in ordeer to make the data realistic."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24dNDuTWSuOQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import math"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxde1nFbUZ_s",
        "colab_type": "code",
        "outputId": "18d2d64f-fdc3-44c6-97ee-76d693eabd0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "def f(x):\n",
        "  return (x - 0.2)*(x + 0.3)*(3*x - 1.5) + 10\n",
        "\n",
        "SIZE = 50\n",
        "x = np.linspace(-2, 2, SIZE, np.float32)\n",
        "y = f(x)\n",
        "\n",
        "# add noise\n",
        "y_noise = y + 2 * np.random.randn(SIZE)\n",
        "\n",
        "# data split into training and testing data\n",
        "idx = list(range(SIZE))\n",
        "np.random.shuffle(idx)\n",
        "\n",
        "x_train = x[idx][:40]\n",
        "y_train = y_noise[idx][:40]\n",
        "\n",
        "x_test = x[idx][40:]\n",
        "y_test = y[idx][40:]\n",
        "\n",
        "# plot train and test points\n",
        "plt.plot(x_train, y_train, 'o')\n",
        "plt.plot(x_test, y_test, 'ro')\n",
        "plt.plot(x, y, 'k-')\n",
        "plt.legend(['Training data', 'Test data', 'Underlying phenomenon'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f020657d550>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3Xt8zvX/+PHHa7PZHDKnxByGkM1h\nY2Qhp4oOn0iEzyLkVInq+1PzcQrJMUlJqRwbCqFylhJKIcNMCzEsOWUO29jp9fvjuna12XXteJ12\nXc/77bbbdV3v9+t6v5/vt8vzel2v9+v9eimtNUIIIVyfh6MDEEIIYR+S8IUQwk1IwhdCCDchCV8I\nIdyEJHwhhHATkvCFEMJNSMIXQgg3IQlfCCHchCR8IYRwEyUcHUBWlSpV0gEBAY4OQwghipUDBw5c\n1lpXzqtckRO+UsoH+BEoadzeaq31BKVUbWAlUBE4APTVWqfktq2AgAD2799f1JCEEMKtKKXi8lPO\nGk06t4GOWuumQDDQRSnVCpgOvKu1vhe4CjxvhX0JIYQopCInfG1w0/jSy/ingY7AauPyJUC3ou5L\nCCFE4Vnloq1SylMpFQVcBLYBJ4EErXWascg5wN/Ce4copfYrpfZfunTJGuEIIYQwwyoXbbXW6UCw\nUsoPWAvcV4D3LgAWAISGhuYYqzk1NZVz585x69Yta4QqRL75+PhQvXp1vLy8HB2KEFZh1V46WusE\npdT3QBjgp5QqYazlVwfiC7PNc+fOUbZsWQICAlBKWTNcISzSWnPlyhXOnTtH7dq1HR2OEFZR5CYd\npVRlY80epZQv8DBwDPge6GEs9hywvjDbv3XrFhUrVpRkL+xKKUXFihXll6VwKdao4VcFliilPDF8\ngXyptf5WKRUDrFRKvQUcBD4r7A4k2QtHkM+dcDVFTvha68NAiJnlfwIti7p9IYRwdZMmTaJLly60\nbGnblClDK+ThypUrBAcHExwczD333IO/v7/pdUpKrveRmQwYMIDY2Nhcy8ybN4/IyEhrhJzN9u3b\n6dYt9x6xv/32G5s3b7b6voUQeduxYwcTJkxg48aNNt+XUw2tYA3rDsYzc0ssfyUkU83Pl1GdG9At\nxGyP0HypWLEiUVFRALz55puUKVOG//f//l+2MlprtNZ4eJj//ly0aFGe+3nppZcKHWNR/fbbb0RH\nR9OlSxeHxSCEO0pJSeHFF1+kTp06vPHGGzbfn0vV8NcdjGf0V0eIT0hGA/EJyYz+6gjrDhaqg1Cu\nTpw4QWBgIOHh4QQFBXH+/HmGDBlCaGgoQUFBTJo0yVS2TZs2REVFkZaWhp+fHxERETRt2pSwsDAu\nXrwIwNixY5kzZ46pfEREBC1btqRBgwb89NNPACQmJvL0008TGBhIjx49CA0NNX0ZZbVhwwYaNGhA\ns2bNWL/+32vle/fuJSwsjJCQEFq3bs3x48dJTk5m0qRJREZGEhwczOrVq82WE0JY3zvvvENsbCwf\nfPABvr6+Nt+fSyX8mVtiSU5Nz7YsOTWdmVtyb04prN9//51XX32VmJgY/P39mTZtGvv37+fQoUNs\n27aNmJiYHO+5du0a7dq149ChQ4SFhbFw4UKz29Za8+uvvzJz5kzTl8f777/PPffcQ0xMDOPGjePg\nwYM53peUlMTQoUPZuHEjBw4c4K+//jKta9iwIbt27eLgwYOMGzeOsWPH4uvry/jx4wkPDycqKooe\nPXqYLSeEsK7Tp08zefJkunfvzqOPPmqXfbpUk85fCckFWl5UdevWJTQ01PR6xYoVfPbZZ6SlpfHX\nX38RExNDYGBgtvf4+vqa/nGbN2/Orl27zG67e/fupjKnT58GYPfu3aaffU2bNiUoKCjH+2JiYqhf\nvz5169YFIDw8nKVLlwKQkJBAv379OHnyZK7Hld9yQojCe+WVV/Dw8DD9srcHl6rhV/Mz/5PI0vKi\nKl26tOn58ePHee+999ixYweHDx+mS5cuZvtwe3t7m557enqSlpaWowxAyZIl8yxTUGPGjKFz585E\nR0ezbt06i33M81tOCFE433zzDevXr2fChAnUqFHDbvt1qYQ/qnMDfL08sy3z9fJkVOcGNt/39evX\nKVu2LHfddRfnz59ny5YtVt9H69at+fLLLwE4cuSI2SajwMBAjh8/zqlTp9Bas2LFCtO6a9eu4e9v\nuIC9ePFi0/KyZcty48aNPMsJ4W7WHYyn9bQd1I7YQOtpO6xyPTApKYkRI0YQGBjIK6+8YoUo88+l\nEn63EH+mdm+Mv58vCvD382Vq98ZF6qWTX82aNSMwMJD77ruPfv360bp1a6vv4+WXXyY+Pp7AwEAm\nTpxIYGAg5cqVy1amVKlSfPTRRzz66KOEhoZStWpV07o33niDUaNG0axZM7T+d9iijh07cujQIUJC\nQli9erXFckK4E1t1Ann77bc5ffo0H374IV5eXjb5UrFEOdN/6NDQUH3nBCjHjh2jYcOGDorIuaSl\npZGWloaPjw/Hjx/nkUce4fjx45Qo4VKXYpyKfP7cV+tpO4g3c/3P38+XPREdC7XN2NhYGjduTO/e\nvVm6dKnpSyVrZxNfL88CV1SVUge01qF5lZNMUYzcvHmTTp06kZaWhtaajz/+WJK9EDZi7U4gWmte\neuklSpUqxcyZM4HcexbaomVCskUx4ufnx4EDBxwdhhBuoZqfr9kafmE7gSxdupTvvvuODz74gCpV\nqgD271noUm34QghhLdbsBBIfH8/IkSNp06YNL7zwgmm5vXsWSsIXQggzCtMJxNwFWK01Q4YMISUl\nhUWLFmUbgsXePQulSUcIISzoFuKf77b0Oy/AZvbq2fH1l2zcuJE5c+Zw77335tg+YNXxv3IjCV8I\nIQrI3CCN5i7A3vjnAh8uHEfbtm15+eWXzW6rIF8qRSVNOnmwxvDIAAsXLuTvv//Os9yJEycIDg7O\ntcyff/7JypUr871vIYT1WOqff+cFXq01Vza/T0ZaGgsXLrQ4mq49OT4Ca4uMhIAA8PAwPBZxjPnM\n4ZGjoqIYNmwYr776qul11mES8pLfhJ8fkvCFcBxLXSk975ghLfHIdm79eYCALoNyNOU4imsl/MhI\nGDIE4uJAa8PjkCFFTvqWLFmyhJYtWxIcHMyLL75IRkYGaWlp9O3bl8aNG9OoUSPmzp3LF198QVRU\nFL169TL7y2Dfvn00adKE4OBgPvroI9PykydP0rZtW0JCQmjevDm//PILABEREXz//fcEBwczd+5c\ni+WEENZnqctkutamC7Bp1y/zz3ef4FuzEbPetP049/nlWgl/zBhISsq+LCnJsNzKoqOjWbt2LT/9\n9JNprPuVK1dy4MABLl++zJEjR4iOjqZfv36mRJ+Z+O/8ZdC/f3/mz59PVFQU6en/1hyqVq3Ktm3b\nOHjwIJGRkYwYMQKAadOm0aFDB6KiohgxYoTFckII67PUZTKzF0+1cj78s/l9lM7gnfc/ontz+w2O\nlhfXSvhnzhRseRFs376dffv2ERoaSnBwMDt37uTkyZPce++9xMbGMmLECLZs2ZJjrJs7Xb58meTk\nZNPYO3379jWtu337Ns8//zyNGjWid+/eZgdLK0g5IUTR5daVsluIP/0rniT51AHmvDODF560/pha\nReFavXRq1jQ045hbbmVaawYOHMjkyZNzrDt8+DCbNm1i3rx5rFmzhgULFhRqH++88w41atTg888/\nJzU1lTJlyhSpnBCi6HLrShkdHc3IkSN5+OGHHTptqSWulfCnTDG02Wdt1ilVyrDcyh566CF69OjB\nyJEjqVSpEleuXCExMRFfX198fHzo2bMn9erVY9CgQUDOIYgzVapUCV9fX37++WfCwsKyTWR+7do1\n7r33XpRSLFmyxDRypbnhjM2VE0LYhrmulElJSfTq1Yty5cqxbNmyHL1yrD3fdmG4VpNOeDgsWAC1\naoFShscFCwzLraxx48ZMmDCBhx56iCZNmvDII49w4cIFzp49y4MPPkhwcDADBgzg7bffBmDAgAEM\nGjTI7EXbRYsWMXToUIKDg7N9SIYPH86nn35K06ZNOXXqlGlSlJCQENLT02natClz5861WE4IYT8j\nR47k2LFjfP7556axcjLZc77t3MjwyELkQj5/Ij9WrlxJnz59GD16tKmSl5UthlrOKr/DI7tWDV8I\nIezs5MmTDBkyhAceeICJEyeaLWPvUTEtkYQvhBCFlJKSQu/evfH09GT58uV4eXmZLWfvUTEtkYQv\nhBCFNHr0aPbv38+iRYuoVauWxXKOnG87K9fqpSOEEHby7bffMnv2bIYPH063bt1yLWvvUTEtkYQv\nhBAFFBsby7PPPktISIhpusK82HNUTEukSUcIIQrg2rVrdO3aFW9vb9auXYuPj4+jQ8o3Sfh5OH36\nNI0aNcq27M0332TWrFkF2k779u25s8tpXvr378/q1atzLfPYY4+RkJBQoO0WlNy5K4RBeno6ffr0\n4eTJk6xZsybXdntnJE06TirrIGq52bhxo40jEUJk+t///semTZv46KOPaNu2raPDKTCp4RdR+/bt\neeONN2jZsiX169dn165dACQnJ9O7d28aNmzIU089RXLyv/1tt27dSlhYGM2aNaNnz57cvHkTgICA\nAN544w2aNWvGqlWrTOV37NiR7aLQtm3beOqpp0zvuXz5MqdPn6Zhw4YMHjyYoKAgHnnkEdM+sw6/\nPGrUqBy/WAB++OEHHnzwQR5//HEaNGjAsGHDyMjIMK0fM2YMTZs2pVWrVly4cAGAS5cu8fTTT9Oi\nRQtatGjBnj17AMMvoIEDB9K+fXvq1KnD3LlzTduZPXs2jRo1olGjRsyZMwcw/Iq677776N+/P/Xr\n1yc8PJzt27fTunVr6tWrx6+//gpAYmIiAwcOpGXLloSEhLB+/XoAFi9eTPfu3enSpQv16tXj9ddf\nN+1vxYoVpqGq33jj32Fqy5QpY/aYhLAkMjKSGTNm8MILLzB06FBHh1M4Wusi/QE1gO+BGOAoMNK4\nvAKwDThufCyf17aaN2+u7xQTE2N6PnLkSN2uXTur/o0cOTLHPrM6deqUDgoKyrZswoQJeubMmVpr\nrdu1a6dfe+01rbXWGzZs0J06ddJaa/3OO+/oAQMGaK21PnTokPb09NT79u3Tly5d0m3bttU3b97U\nWms9bdo0PXHiRK211rVq1dLTp0837ee5557Tq1at0hkZGbpBgwb64sWLWmut+/Tpo7/++mvTey5d\nuqRPnTqlPT099cGDB7XWWvfs2VMvW7ZMa611UFCQ/umnn7TWWr/xxhs5jkdrrb///ntdsmRJffLk\nSZ2WlqYfeughvWrVKq0N/5im/Y0aNUpPnjzZFMeuXbu01lrHxcXp++67z3R+wsLC9K1bt/SlS5d0\nhQoVdEpKit6/f79u1KiRvnnzpr5x44YODAzUv/32myn2w4cP6/T0dN2sWTM9YMAAnZGRodetW6e7\ndu2qtdZ69OjRpmO6evWqrlevnr5586ZetGiRrl27tk5ISNDJycm6Zs2a+syZMzo+Pl7XqFFDX7x4\nUaempuoOHTrotWvX5npMd8r6+RPua9++fdrHx0e3a9dOp6SkODqcHID9Oh/52ho1/DTg/7TWgUAr\n4CWlVCAQAXynta4HfGd8XeyoO2axMbe8e/fuADRv3pzTp08D8OOPP/Lss88C0KRJE5o0aQLA3r17\niYmJoXXr1gQHB7NkyRLisozw2atXL7P76tu3L59//jkJCQn8/PPPPProoznK1a5d2zQ9YmYsCQkJ\n3Lhxg7CwMAD++9//WjzWli1bUqdOHTw9PenTpw+7d+8GwNvbmyeeeCLHMW7fvp3hw4cTHBzMk08+\nyfXr102/Vh5//HFKlixJpUqVuPvuu7lw4QK7d+/mqaeeonTp0pQpU4bu3bubfhHVrl2bxo0b4+Hh\nQVBQEJ06dUIpRePGjU3727p1K9OmTSM4OJj27dtz69YtzhiHvu7UqRPlypXDx8eHwMBA4uLi2Ldv\nH+3bt6dy5cqUKFGC8PBwfvzxx1yPSYg7nT9/nm7dulGlShVWrVpl8eaq4qDIbfha6/PAeePzG0qp\nY4A/0BVobyy2BPgBKNLUL5lNAPZUsWJFrl69mm3ZP//8Q+3atU2vMwcr8/T0JC0tLdftaa15+OGH\nWbFihdn1pUuXNrt8wIAB/Oc//zGNxFmiRM5/uqyDpnl6emZrRsqPO7/cMl97eXmZnmc9xoyMDPbu\n3Wu2l8KdseR1XrKW9/DwML328PAwvVdrzZo1a2jQIPvNKr/88kuB92fpmITIKjExkaeeeoqrV6/y\n008/UblyZUeHVCRWbcNXSgUAIcAvQBXjlwHA30AVC29zamXKlKFq1ars2LEDMCT7zZs306ZNm1zf\n9+CDD7J8+XLAMDvW4cOHAWjVqhV79uzhxIkTgOED9ccff+QZR7Vq1ahWrRpvvfUWAwYMyHf8fn5+\nlC1b1jTtYW5z4f7666+cOnWKjIwMvvjiizyP8ZFHHuH99983vY6Kisq1fNu2bVm3bh1JSUkkJiay\ndu3aAl346ty5M++//75p+OeDBw/mWr5ly5bs3LmTy5cvk56ezooVK2jXrl2+9yfcW2pqKj179mTf\nvn1ERkbStGlTR4dUZFZL+EqpMsAa4BWt9fWs64xtTGaH5VRKDVFK7VdK7b906ZK1wrGqpUuXMnny\nZIKDg+nYsSMTJkygbt26ub7nhRde4ObNmzRs2JDx48fTvHlzACpXrszixYvp06cPTZo0ISwsjN9/\n/z1fcYSHh1OjRo0Cj9742WefMXjwYIKDg0lMTLQ4C1eLFi0YPnw4DRs2pHbt2qYLw5bMnTuX/fv3\n06RJEwIDA7PNx2tOs2bN6N+/Py1btuT+++9n0KBBhISE5Ps4xo0bR2pqKk2aNCEoKIhx48blWr5q\n1aqm6SCbNm1K8+bN6dq1a773J9xXRkYGAwcOZNOmTXz88cd53klbbOSnoT+vP8AL2AK8lmVZLFDV\n+LwqEJvXdvK6aOvuXnrpJf3pp58W+H03btwwPZ86daoeMWJEjjLff/+9fvzxx4sUnyuSz5/7ycjI\n0K+99poG9FtvveXocPIFe120VYaG0M+AY1rr2VlWfQ08Z3z+HLC+qPtyZ82bN+fw4cOmC8EFsWHD\nBoKDg2nUqBG7du1i7NixNohQCNcwc+ZMZs+ezcsvv8z//vc/R4djVUWeAEUp1QbYBRwBMjtu/w9D\nO/6XQE0gDnhGa/1PbtuSCVCEs5HPn3tZtGgRAwcOpHfv3kRGRuaYptBZ5XcCFGv00tkNmO+7CJ2K\nun3jPix2jxTCVopaGRLFyzfffMPgwYN5+OGHWbJkSbFJ9gXh9Efk4+PDlStX5D+fsCutNVeuXClW\nA2OJwtu6dSvPPPMMISEhrFmzBm9vb0eHZBNOP5ZO9erVOXfuHM7ag0e4Lh8fH6pXr+7oMISNbdiw\nge7du9OwYUM2bdpE2bJlHR2SzTh9wvfy8sp2k5MQQljL+vXr6dmzJ02aNGHr1q1UqFDB0SHZlNM3\n6QghhC2sWbOGHj16EBISwvbt210+2YMkfCGEG/riiy/o1asXLVq0YOvWrfj5+Tk6JLuQhC+EcCuf\nf/45//3vf3nggQfYsmWLxTvPXZHTt+ELIURBrDsYb3Gy8A8++IARI0bQvn17vvnmG4uDFboqSfhC\nCJex7mA8o786QnKqYca4+IRkRn91hPT0dPYsn8O7777Lk08+yYoVKyhVqpSDo7U/SfhCCJcxc0us\nKdlnSkxKZEj/cP45upsRI0Ywe/ZsPD09HRShY0nCF0K4jL8Sss8BkZ54lYtrJpNy/jhz5sxh5MiR\nZt+XWzOQK5GEL4RwGdX8fIk3Jv3UK2e5uOpN0hMTuK/vREaOHGk2sQNmm4EAl0v6RR48zZrMDZ4m\nhBD5ldmG/8/xA1xeNxU8vajR+03mDO8BZE/sAL5enpQs4UFCcmqObfn7+bInoqPdYi8Kuw2eJoQQ\njmCutv6fJvfwxcezWfnlu3hVrEHjAW8zrk97uoX403rajhzt+8mp6TmWZbqzecgVSMIXQhQ75nrj\njFq2i4k/f0TU3p307duX+fPnZ+t2WdAEXs3P16oxOwNJ+MIpuctFNFE4d/bGuXUuhnPrp5Nx6zoL\nFixg0KBBOYZUz9q+n1X5Ul7cSs3I0dST2b7vSuROW+F0Mmtv8QnJaP69iLbuYLyjQxNOIrO2rrXm\n+q9fcWHFaFQJb+55dhaDBw82O3/GqM4N8PXK3h3T18uTCf8JYmr3xvj7+aIwtN1P7d7YJSsYUsMX\nTsdcX+rk1HRmbol1yf+EouCq+fkSd/Ys/2yZR/LJfZSq/wAVHxtJjSqVLL4n87Nj6ZejO3y2JOEL\np2OprdUVL6KJgtNa0+xWFHs/G49OT6P8Q0Mp2+wJSnmXyLMZpluIv1skdksk4QunY6mt1RUvool8\niIyEMWPgzBnOVqvGkIoV2Xz4MIHN7qdkh5e4WqKCXOfJJ0n4wumM6tzAbH9pV7yIJvIQGQlDhqCT\nkvgMeC0+nvT4eN7v148XFy1yyXlnbUkSvnA6ebW1CjcyZgzRSUmMBHYAHYBPgTo7d4Ik+wKThC+c\nkru3tZrjbl1VL126xPi4OBYA5YD5wBCMXQvPnHFkaMWWJHwhigFLw/6CfXqX2PPL5vbt27z//vtM\nnjyZRGA4MAHINgFhzZo22berk99EQhQDuXVVtTV73Rehtearr74iKCiIUaNG0aZNG45Mn857pUpl\nT/alSsGUKVbdt7uQhC9EMeDIrqq2/rJJT09n1apVhISE8PTTT+Pt7c2mTZvYsGEDDV9/HRYsgFq1\nQCnD44IFEB5ulX27G0n4QhQDlrqk2qOrqq2+bNLS0li2bBmNGjXimWeeITk5mcWLF3Po0CG6dOny\nb8HwcDh9GjIyDI+S7AtN2vCFRc56kdBZ47IWc8fnyK6q1r4vIjk5mc8//5xp06bx559/0rhxY1au\nXEmPHj3cdiYqe5EavjDLWcezcda4rMXS8QEOG+/F0hg0Bf2y+f3333n11Vfx9/dnyJAhVKxYkfXr\n1xMVFUWvXr0k2duB1PCFWc46no2zxmUtuR3fnoiODjnGwtwXkfkrJf7ydbzP7cf7xHccPbAXLy8v\nunfvzrBhw2jXrp3ZQc6E7UjCF2Y563g2zhqXtTjr8Vm8LyLLsAfUrAlTprCmQVtem/sFV4/+SOKx\nH8lIvo6X3z30HTGaWWNe4e6777b/AQhAEr6wwFnHs7FHXI68RuCs590s47AHJCWRDuyOi2NV//4s\nKVGSm7cSUSW88a3bgjJNO+MTEMzJ8qUl2TuYJHxhlrOOZ2PNuJxxQmtnPe/mXI6IYEdSEtuAb4G/\nAd+0NDrgwa9Pvo5v3RZ4eP/7ReXoXylCEr6wwFnHs7FWXJbuXPXx8nDoNQJrnndr/1JJSkpi9+7d\nbN++ne3bt3Pw3DnAMOzBw0BP4DGgVFoqdRo+mOP9Tvkrxc1YJeErpRYCTwAXtdaNjMsqAF8AAcBp\n4Bmt9VVr7E/Yh7OOZ2ONuCxdHHWGCa2tcXy5DsUQ80OOdvc7+7anpqYSHR3Nvn37TH/R0dGkp6fj\n5eVF69atecvPj4cSEmhO9kSSVLUavl6exeJXiruxVrfMxUCXO5ZFAN9presB3xlfC+EUCj2hdWQk\nBAQYRmoMCDC8zm25JQUtX8BtWfpCi5o2z9DuHhcHWpMSF8exQYNY9+qrTJs2jQEDBnD//fdz1113\n0axZM4YOHcqaNWuoUqUKERERbN68matXr/L9998z5oMPuL9Uqey1xlKlKDVzusUupOsOxtN62g5q\nR2yg9bQd2brT5rZOWIfSWltnQ0oFAN9mqeHHAu211ueVUlWBH7TWuX7Fh4aG6v3791slHmFblpoL\nisNNUVprHpiylfirNyEjA60zAFBAOV9vbqdncCs1A5QHysMT35JeTOsRbKgZGy9SmpQqBc89B0uW\n5FxuaQiALBc781U+Nxa2NaLDMNY1eICM5JtkJF8j7cZl0m9cIfyHxVxPSeIccAb4E8j6tVC1alXu\nu+8+QkJCaNGiBS1atKBOnTqWu0+a6aVj6Rju/NUBhpr/1O6NASyuc7bPjzNSSh3QWofmWc6GCT9B\na+1nfK6Aq5mvLZGEXzxY+o/7dHN/1hyIt+l/Wq01CQkJnDlzhkuXLnH58mXTY+bftWvXSExMJDEx\nkZs3b5oeb926RVpaGhkZGQXer1IKL60pAXgB3kDJOx4zn5v+fH0p2bUrJUuWxNvbGy8vL8PjwoV4\nXb+ON4amEE8MP7U9/PzwGDcODw8PPDw8yMjIID093fSX+frWrVskJyeTlJREUmQkyYmJJAHXgavA\nP8a/FDPH4QlUA6ob/xoY/+4D6l+7xl133VXgc5NfraftMNsDyd/468nSuj0RHW0Wk6vIb8K3y0Vb\nrbVWSpn9ZlFKDcEwzDU1ZcjTYsFSc8GKX86SfkcFojAXPNPT0zl9+jRHjx7l999/5/Tp08TFxXHm\nzBni4uK4ceNGjvcopahQoQIVK1bEz8+P0qVL4+/vT5kyZShdujSlS5fGx8cHLy8vvLy8KFGiBH9c\nTOLHE/9w7VY65Xy96NCgMo3870JrjdbalGBTU1NJS0sjdfJk0oBUDMn0dpbH21leJwPXgNvJydz+\n7Tdu375NSkoKqamphsfr10kF0u48iIQE+L//y/P8eHl54evri6+vL6USE/EFfIGyGBJ3BaA8sLjD\nANK9S+PpexeeZStSukIVDn7xOmX//ivnRmvVAhsmeyjcPQbSs8e6bJnwLyilqmZp0rlorpDWegGw\nAAw1fBvGI6zE0n/CO5N9tvIWfvpfv36dX375hQMHDnD06FFTkk9O/ncfFSpUoGbNmtStW5eOHTtS\nq1YtatSowT333EOlSpWoXLky5cuXt/2t+UuXGtq+7+TpCelmLvbWqgWxZkaUDAiAuDg0hi8PjaFZ\nJaNGDTKio8nIyCAjIwMPDw88PT3x9PTM8fzObZnb9wPvTM7RvFa2xQzzzUl2GG44r3sMis39B8WY\nLRP+18BzwDTj43ob7kvYkaX/uJ5KmU36z53aA++/R0ZSErHAz3Fx/Pzcc+yNiOBofDyZzYrVq1cn\nKCiIDh06EBgYSFBQEA0bNqThftDTAAAV+UlEQVRcuXK2PqT8mTKlYG34lpKocTsqKQnvrOWnTi14\nLdtSTFOmmO/tE2JsX7fQ7m7LazB53WNQXO4/KM6s1S1zBdAeqKSUOodhgpppwJdKqeeBOOAZa+xL\nOJ6l/7jm2vC9Um5Qa+tH9E1KYgtwybi8fHo6rS5fpuebbxIWFkaLFi3w88v1Ek+hWS2JheeSLFu3\nzvfFy1y3Y82YcnuPmfW2nlUrP/cYOPsF/+LOahdtrUEu2hYflpLo2t/OMeGzrzkTtYv0MwdJjP8D\nrTWVgM5AJyAMqA94KGUY49zGcTqy90dx6LWUKbeLqnLh1Lk51UVb4XrubC6IiYlh7Nj5LF++nFOn\nTuHh4cH999/Po0Mm0uXDD2n+9985b/qww0V6R46u6eh5aAvKWQduE9YjCV8U2pkzZ1ixYgUrVqzg\n0KFDeHh40KlTJ8aPH8+TTz5JhQrGmUjr1HHYhUJnnRrQGRN+sRq4TRSKTIAiCiQjI4MtW7bwxBNP\nEBAQQEREBL6+vsydO5e//vqLrVu30r9//3+TPRjaix00L6krTg1oK9aa6EQ4L6nhi3y5fv06ixcv\nZt68efzxxx9UqVKFsWPH0r9/f+rUqZP3BnK5UGjLNm5XmhrQ1px1wDxhPZLwRa5OnTrFrFmzWLp0\nKTdv3qRVq1ZERkbSo0cPvL29895ALuzRxu3IJFachjrO5KwD5gnrkITvgqxRa46Li+Ott95i8eLF\neHh40KdPH4YPH05oaJ4dAfLNXm3cjkpiUmMWzkYSvospaq357NmzTJkyhYULF6KU4oUXXiAiIoJq\n1apZPdbi1sZdGFJjFs5EEr6LKWyt+fz580yZMoVPPvkErTWDBw9m9OjRVK9e3WaxWruNuzj1eRfC\nEaSXjospaK05JSWFWbNmUb9+fT7++GP69+/PiRMnmDdvnk2TPVi3V0jmL5v4hGQ0//6ykTHVhfiX\nJHwXU5BuiNu2baNp06aMGjWK9u3bc+zYMT7++GO7jVraLcTf4kQZBZXbLxshhIE06biY/PQMiYuL\n47XXXuOrr76ibt26fPvttzz++OOOCNdqbdzucD1AiKKSGr6Lya3WnJaWxttvv03Dhg3ZtGkTU6ZM\nITo62mHJ3poceYOVEMWF1PBdkLla8x9//EG/fv345ZdfePrpp5k9e7ZLTThTHPu8C2FvUsN3cRkZ\nGcybN4/g4GCOHvud+n3GcuDeAfRZfsKlLmha83qAEK5Kavgu7Ny5cwwcOJBt27bRrHUHroU+z20f\nw5jzzj5yY2FIn3chcic1fBe1fPlyGjVqxJ49e/joo48o+fgY0nyyTzAivViEcC+S8F1MSkoKL774\nIuHh4QQFBXHo0CGGDh3K+Wu3zJaXXixCuA9J+C4kPj6edu3aMX/+fF5//XV27tzJvffeC0gvFiGE\nJPziLTISAgLAw4Mf77mH5kFBHDlyhFWrVjF9+nRKlPj3Eo2MdS6EkIu2xVVkJAwZgk5K4n3g/y5c\noI5S7Jg2jcAePXIUl5EbhRAyiXlxFRBAclwcg4FIoCuwBChXqxacPu3Q0IQQ9iWTmLu4f+LieBL4\nCZgM/A9j+9yZMwXelowyKYR7kIRfDJ09e5YuXl6cSE3lC6Bn1pUFvHvWHrNOCSGcg1y0LWaio6MJ\nCwvjXIkSbClZMnuyL1UKpkwp0PZklEkh3Ick/GJk586dtGnTBq01u/bupf1nn0GtWqCU4XHBArMT\nhedGRpkUwn1Ik46dFba9fM2aNYSHh1O7dm02b95MrVq1oEmTAif4O1l71ikhhPOSGr4dFXZWps8+\n+4yePXvSvHlzdu/ebUj2ViL984VwH5Lw7agw7eULFy5k0KBBdO7cme3bt1OxYkWrxiSjTArhPqRJ\nx44K2l6+ZMkSU7Jfu3YtPj4+NolLRpkUwj1IDd+OCjKezbJlyxgwYAAPPfSQTZO9EMJ9SMK3o/y2\nl0dGRvLcc8/RoUMH1q1bh6+vXEAVQhSdNOnYUX7Gs1mxYgX9+vWjXbt2fPPNN2yNvcrMLXvlLlgh\nRJFJwrez3NrLV69ezbPPPkubNm349ttv2Rp7Ve6CFUJYjTTpOImdO3cSHh5Oq1at2LBhA6VLl5a7\nYIUQVmXzhK+U6qKUilVKnVBKRdh6f8XR0aNH6datG3Xq1OGbb76hTJkygNwFK4SwLpsmfKWUJzAP\neBQIBPoopQJtuc/iJj4+nkcffRQfHx82bdpEhQoVTOtkliohhDXZuobfEjihtf5Ta50CrMQwdLsA\nrl+/zmOPPcbVq1fZuHEjAQEB2dbLXbBCCGuy9UVbf+BsltfngPuzFlBKDQGGANQs4NC+xVlKSgpP\nP/00MTExbNiwgZCQkBxlZJYqIYQ1ObyXjtZ6AbAADDNeOTgcu9BaM2jQILZv386iRYt45JFHLA6q\nJnfBCiGsxdYJPx6okeV1deMytzZ+/HiWLVvGpEmT6N+/v0xCIoSwC1u34e8D6imlaiulvIHewNc2\n3qdTW7VqFW+99RbPP/88Y8eOBWQSEiGEfdi0hq+1TlNKDQe2AJ7AQq31UVvu05lFR0czYMAAwsLC\n+PDDD1FKAdL9UghhHzZvw9dabwQ22no/zi4hIYGnnnqKsmXLsnr1ary9vU3rZBISIYQ9yJ22dpCR\nkUHfvn05ffo0q1evplq1atnWS/dLIYQ9OLyXjjuYNGkS3377LfPmzaN169Y51kv3SyGEPSitnacn\nZGhoqN6/f7+jw7Cqr7/+mq5du9K/f38WLlxoarcXQghrUUod0FqH5lVOmnRsKDY2lr59+xIaGsr8\n+fMl2QshHEoSvo0kJibSvXt3vL29WbNmjcxYJYRwOGnDt5FXX32VY8eOsW3bNrcaMkII4bykhm8D\na9as4ZNPPiEiIoJOnTo5OhwhhAAk4Vvd2bNnGTx4MC1atGDixImODkcIIUwk4VtReno6/fr1IyUl\nheXLl+Pl5eXokIQQwkTa8K1oxowZ/PDDDyxevJh7773X0eEIIUQ2UsO3kl9//ZXx48fTu3dv+vXr\n5+hwhBAiB6nhF0HmGPbnLlzh4tJXKF/5HulvL4RwWlLDL6TMMezjE5K5sv1jbl39G9+HR/LDqURH\nhyaEEGZJDb+QMsewT4zdQ2L0d5Rr3QdVtSEzt8TSLcTf4gxWQgjhKJLwC+mvhGTSk2/wz7b5eFep\nS7kHepuWywxWQghnJE06hVTNz5erOz4hI/kGFR8bifLwNC2XGayEEM5IEn4hdSwTT2L0Dsrd3wPv\nu+sA/45hLzNYCSGckST8Qrhx4wZLZoyhRp36NHysPwrw9/NlavfGdAvxtzhTlcxgJYRwJGnDL4SI\niAjOnTvHTz/9RKtWrXKsH9W5QbY2fJAZrIQQjicJv4B+/PFHPvzwQ1555RWzyR5kBishhHOSGa8K\nIDk5maZNm5Kens7hw4cpXbq0o0MSQoh8z3glNfwCePPNNzl+/DjfffedJHshRLEjF23z6cCBA8ya\nNYtBgwbRsWNHR4cjhBAFJgk/HzIyMhg2bBhVqlRh5syZjg5HCCEKRZp08mHhwoXs37+fyMhI/Pz8\nHB2OEEIUitTw8/DPP/8QERFB27Zt6dOnj6PDEUKIQpOEn4fx48dz9epVPvjgAxn2WAhRrEmTTi6i\noqKYP38+jz7zHC9svMxfyzdIn3ohRLElCd8CrTXDhw+nTLny/FH9MVKM4+DIyJdCiOJKmnQs+Pzz\nz9mzZw+VOw4kxTP7GDgy8qUQojiShG/G9evXGTVqFPfffz9pdduaLSMjXwohihtJ+FlFRkJAABPL\nlePihQt88Pjj+Jc3f0etjHwphChuJOFnioyEIUM4GhfHe8BgIHTaNOakx+Dr5ZmtqIx8KYQojoqU\n8JVSPZVSR5VSGUqp0DvWjVZKnVBKxSqlOhctTDsYMwaSkngVuAuYApCURItPZjG1e2P8/XxzjHsv\nhBDFSVF76UQD3YGPsy5USgUCvYEgoBqwXSlVX2udnnMTTuLMGbYC24B3gUpZlncL8ZcEL4Qo9opU\nw9daH9Nam+uu0hVYqbW+rbU+BZwAWhZlX7aWUaMGrwO1gReyrqhZ0zEBCSGEldmqDd8fOJvl9Tnj\nMqcV+dhjHMLQlFMyc2GpUjBliuOCEkIIK8qzSUcptR24x8yqMVrr9UUNQCk1BBgCUNNBtelbt24x\nduNGmteuTa/0dDh71lCznzIFwsMdEpMQQlhbnglfa/1QIbYbD9TI8rq6cZm57S8AFoBhxqtC7KvI\n5s2bx5kzZ1j03Xd4yFj3QggXZasmna+B3kqpkkqp2kA94Fcb7atIrl69ypQpU+jSpYtMbCKEcGlF\n7Zb5lFLqHBAGbFBKbQHQWh8FvgRigM3AS87aQ2fq1KkkJCQwffp0R4cihBA25daTmJ85c4b69evT\nu3dvFi9ebLf9CiGENeV3EnO3vtN23LhxAEyaNMnBkQghhO25bcI/dOgQy5YtY8SIEQ7rHSSEEPbk\ntgk/IiICPz8/Ro8e7ehQhBDCLtxyApQ9e/awefNmpk+fTvny5R0djhBC2IVb1vAnTpxI5cqVeeml\nlxwdihBC2I3b1fD37NnDtm3bmDlzJqVLmx/rXgghXJHb1fAnTpzI3XffzQsvvJB3YSGEcCFuVcPP\nrN3PmjVLavdCCLfjVjX8zNr9sGHDHB2KEELYXbGv4a87GM/MLbH8lZBMNT9fRnVuYHayEqndCyHc\nXbFO+OsOxjP6qyMkpxqG6YlPSGb0V0cAciT9N998U2r3Qgi3VqybdGZuiTUl+0zJqenM3JJ9Eq7d\nu3ezfft2Xn/9dandCyHcVrFO+H8lJOdrubTdCyFEMW/SqebnS7yZpF/Nz9f0PLN2/84775hq9/lt\n9xdCCFdSrGv4ozo3wNfLM9syXy9PRnVuYHp9Z+0+s90/PiEZzb/t/usOmp2QSwghXEaxTvjdQvyZ\n2r0x/n6+KMDfz5ep3Rubaut79+41td2XKlUKyH+7vxBCuJpi3aQDhqRvqTlmxowZlC9fnqFDh5qW\n5bfdXwghXE2xT/iWxMbGsm7dOvzbh9P4rZ2mtvr8tPsLIYQrKtZNOrl5efQk8CgBQV2ytdV3uK9y\nnu3+Qgjhilwy4f/9999s/3oVpRs/hGdpP9Py5NR0vv/9Uq7t/kII4apcsknnvffeQ2ekc1fLp3Ks\n+yshOdd2fyGEcFUuV8O/fv068+fPp2KjtniVr5ZjvbTVCyHclcsl/E8++YRr167xxuujpK1eCCGy\ncKmEn5KSwrvvvkuHDh0Y9ezj0lYvhBBZuFQb/ooVK4iPj+fTTz8Fcu+jL4QQ7sZlavgZGRnMmDGD\nxo0b07lzZ0eHI4QQTsdlavgbN24kJiaGZcuWoZRydDhCCOF0XKaGP2PGDGrWrEmvXr0cHYoQQjgl\nl6jh//zzz+zatYs5c+bg5eXl6HCEEMIpuUwNv3Pnzjz//POODkMIIZyWS9Tww8LC2Lx5s6PDEEII\np+YyNXwhhBC5k4QvhBBuQhK+EEK4iSIlfKXUTKXU70qpw0qptUopvyzrRiulTiilYpVScieUEEI4\nWFFr+NuARlrrJsAfwGgApVQg0BsIAroAHyqlPC1uRQghhM0VKeFrrbdqrdOML/cC1Y3PuwIrtda3\ntdangBNAy6LsSwghRNFYsw1/ILDJ+NwfOJtl3TnjshyUUkOUUvuVUvsvXbpkxXCEEEJklWc/fKXU\nduAeM6vGaK3XG8uMAdKAyIIGoLVeACwACA0N1QV9vxBCiPzJM+FrrR/Kbb1Sqj/wBNBJa52ZsOOB\nGlmKVTcuy9WBAwcuK6Xi8ipnQSXgciHfa0vOGhc4b2wSV8FIXAXjinHVyk8h9W+OLjilVBdgNtBO\na30py/IgYDmGdvtqwHdAPa11eqF3lncs+7XWobbafmE5a1zgvLFJXAUjcRWMO8dV1KEVPgBKAtuM\nQxLv1VoP01ofVUp9CcRgaOp5yZbJXgghRN6KlPC11vfmsm4KMKUo2xdCCGE9rnSn7QJHB2CBs8YF\nzhubxFUwElfBuG1cRWrDF0IIUXy4Ug1fCCFELoptws9tHJ87ynUxjudzQikVYYe4eiqljiqlMpRS\nFq+4K6VOK6WOKKWilFL7nSguu54v4z4rKKW2KaWOGx/LWyiXbjxfUUqpr20US67Hr5QqqZT6wrj+\nF6VUgC3iKERc/ZVSl7Kcn0F2imuhUuqiUirawnqllJprjPuwUqqZk8TVXil1Lcv5Gm+nuGoopb5X\nSsUY/z+ONFPGdudMa10s/4BHgBLG59OB6WbKeAIngTqAN3AICLRxXA2BBsAPQGgu5U4Dlex4vvKM\nyxHny7jfGUCE8XmEuX9L47qbNo4jz+MHXgQ+Mj7vDXxhh/OTn7j6Ax/Y6/OUZb8PAs2AaAvrH8Nw\nB74CWgG/OElc7YFvHXC+qgLNjM/LYhiD7M5/S5uds2Jbw9eWx/HJqiVwQmv9p9Y6BViJYZwfW8Z1\nTGsda8t9FEY+47L7+TLqCiwxPl8CdLPDPs3Jz/FnjXU10EkZ+yQ7OC6H0Fr/CPyTS5GuwFJtsBfw\nU0pVdYK4HEJrfV5r/Zvx+Q3gGDmHnbHZOSu2Cf8OWcfxySrfY/o4gAa2KqUOKKWGODoYI0edrypa\n6/PG538DVSyU8zGOu7RXKWWLL4X8HL+pjLHCcQ2oaINYChoXwNPGJoDVSqkaZtY7gjP/HwxTSh1S\nSm0y3ixqV8bmwBDglztW2eycOfWctrYex8eWceVDG611vFLqbgw3rv1urJU4Oi6byC22rC+01lop\nZanrWC3jOasD7FBKHdFan7R2rMXUN8AKrfVtpdRQDL9COjo4Jmf2G4bP002l1GPAOqCevXaulCoD\nrAFe0Vpft9d+nTrh68KN45NVocb0KWpc+dxGvPHxolJqLYaf7UVK+FaIyybnC3KPTSl1QSlVVWt9\n3vjT9aKFbWSesz+VUj9gqB1ZM+Hn5/gzy5xTSpUAygFXrBhDoeLSWmeN4VMM10Wcgc0+U0WRNclq\nrTcqpT5USlXSWtt8jB2llBeGZB+ptf7KTBGbnbNi26SjDOP4vA48qbVOslBsH1BPKVVbKeWN4SKb\nTXp3FIRSqrRSqmzmcwwXoM32JrAzR52vr4HnjM+fA3L8GlFKlVdKlTQ+rwS0xjB0hzXl5/izxtoD\n2GGhsmHXuO5o430SQ9uwM/ga6GfsedIKuJal+c5hlFL3ZF57UUq1xJALbf3FjXGfnwHHtNazLRSz\n3Tmz91Vqa/1hmFTlLBBl/MvsOVEN2Jil3GMYroSfxNC0Yeu4nsLQ5nYbuABsuTMuDL0tDhn/jjpL\nXI44X8Z9VsQwwN5xYDtQwbg8FPjU+PwB4IjxnB0BnrdRLDmOH5iEoWIB4AOsMn7+fgXq2Okc5RXX\nVONn6RDwPXCfneJaAZwHUo2fr+eBYcAw43oFzDPGfYRceq7ZOa7hWc7XXuABO8XVBsP1u8NZctdj\n9jpncqetEEK4iWLbpCOEEKJgJOELIYSbkIQvhBBuQhK+EEK4CUn4QgjhJiThCyGEm5CEL4QQbkIS\nvhBCuIn/D6FItdtnAd/cAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQ6dMU7_OnT6",
        "colab_type": "text"
      },
      "source": [
        "PyTorch uses ``torch.tensor``, rather than numpy arrays, so we need to\n",
        "convert our data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMFrWE5VOnT6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train, y_train, x_test, y_test = map(\n",
        "    torch.FloatTensor, (x_train[:, np.newaxis], y_train[:, np.newaxis], x_test[:, np.newaxis], y_test[:, np.newaxis])\n",
        ")\n",
        "n, _ = x_train.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QI6mmsdkLGFC"
      },
      "source": [
        "## Loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xMbpRMDUR92",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mse(inp, target):\n",
        "  return torch.mean((inp - target)**2)\n",
        "  \n",
        "loss_func = mse"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-CN1LxWSLLnf"
      },
      "source": [
        "### Using torch.nn.functional\n",
        "\n",
        "We will now refactor our code, so that it does the same thing as before, only\n",
        "we'll start taking advantage of PyTorch's ``nn`` classes to make it more concise\n",
        "and flexible. At each step from here, we should be making our code one or more\n",
        "of: shorter, more understandable, and/or more flexible.\n",
        "\n",
        "The first and easiest step is to make our code shorter by replacing our\n",
        "hand-written activation and loss functions with those from ``torch.nn.functional``\n",
        "(which is generally imported into the namespace ``F`` by convention). This module\n",
        "contains all the functions in the ``torch.nn`` library (whereas other parts of the\n",
        "library contain classes). As well as a wide range of loss and activation\n",
        "functions, you'll also find here some convenient functions for creating neural\n",
        "nets, such as pooling functions. (There are also functions for doing convolutions,\n",
        "linear layers, etc, but as we'll see, these are usually better handled using\n",
        "other parts of the library.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yB2jostpLLng",
        "colab": {}
      },
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "loss_func = F.mse_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ox_TeIdHUshf",
        "colab_type": "text"
      },
      "source": [
        "## Model definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVwYBgGSTZVe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Mnist_mlp_classifier():\n",
        "  def __init__(self):\n",
        "    self.weights_1 = torch.randn(1, 800)\n",
        "    self.weights_1.requires_grad_()\n",
        "    self.bias_1 = torch.zeros(1, requires_grad=True)\n",
        "\n",
        "    self.weights_2 = torch.randn(800, 1)\n",
        "    self.weights_2.requires_grad_()\n",
        "    self.bias_2 = torch.zeros(1, requires_grad=True)\n",
        "  \n",
        "  def forward(self, xb):\n",
        "    h =  xb.matmul(self.weights_1) + self.bias_1\n",
        "    h = F.tanh(h)\n",
        "    h =  h.matmul(self.weights_2) + self.bias_2\n",
        "    out = h\n",
        "    return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0H2FqBORAotb",
        "colab_type": "text"
      },
      "source": [
        "### Refactor using nn.Module\n",
        "Next up, we'll use ``nn.Module`` and ``nn.Parameter``, for a clearer and more\n",
        "concise training loop. We subclass ``nn.Module`` (which itself is a class and\n",
        "able to keep track of state).  In this case, we want to create a class that\n",
        "holds our weights, bias, and method for the forward step.  ``nn.Module`` has a\n",
        "number of attributes and methods (such as ``.parameters()`` and ``.zero_grad()``)\n",
        "which we will be using."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JK3hV6uAiaK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch import nn\n",
        "\n",
        "class Mnist_mlp_classifier(nn.Module):\n",
        "  def __init__(self):\n",
        "      super().__init__()\n",
        "      self.weights_1 = nn.Parameter(torch.randn(1, 800) / math.sqrt(1))\n",
        "      self.bias_1 = nn.Parameter(torch.zeros(800))\n",
        "      \n",
        "      self.weights_2 = nn.Parameter(torch.randn(800, 1) / math.sqrt(784))\n",
        "      self.bias_2 = nn.Parameter(torch.zeros(1))\n",
        "\n",
        "  def forward(self, xb):\n",
        "    h =  xb.matmul(self.weights_1) + self.bias_1\n",
        "    h = F.tanh(h)\n",
        "    h =  h.matmul(self.weights_2) + self.bias_2\n",
        "    out = h\n",
        "    return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZ-yAitFArg-",
        "colab_type": "code",
        "outputId": "cbc1800c-c2d8-4b8a-a919-4780c4711016",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "model = Mnist_mlp_classifier()\n",
        "print('===========\\nParameters shape\\n===========')\n",
        "for p in model.parameters():\n",
        "  print(p.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "===========\n",
            "Parameters shape\n",
            "===========\n",
            "torch.Size([1, 800])\n",
            "torch.Size([800])\n",
            "torch.Size([800, 1])\n",
            "torch.Size([1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Or3E7_h4Brvl",
        "colab_type": "text"
      },
      "source": [
        "### Refactor using nn.Linear\n",
        "\n",
        "We continue to refactor our code.  Instead of manually defining and\n",
        "initializing ``self.weights`` and ``self.bias``, and calculating ``xb  @\n",
        "self.weights + self.bias``, we will instead use the Pytorch class\n",
        "`nn.Linear <https://pytorch.org/docs/stable/nn.html#linear-layers>`_ for a\n",
        "linear layer, which does all that for us. Pytorch has many types of\n",
        "predefined layers that can greatly simplify our code, and often makes it\n",
        "faster too."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQCOFCZRBvt1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Mnist_mlp_classifier(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.layer1 = nn.Linear(1, 800)\n",
        "    self.layer2 = nn.Linear(800, 1)\n",
        "\n",
        "  def forward(self, xb):\n",
        "    h =  self.layer1(xb)\n",
        "    h = F.tanh(h)\n",
        "    h =  self.layer2(h)\n",
        "    out = h\n",
        "    return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVuGZlRgCLX9",
        "colab_type": "code",
        "outputId": "3ef3cb48-36f0-4f13-be51-d4f266d534af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "model = Mnist_mlp_classifier()\n",
        "print('===========\\nParameters shape\\n===========')\n",
        "for p in model.parameters():\n",
        "  print(p.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "===========\n",
            "Parameters shape\n",
            "===========\n",
            "torch.Size([800, 1])\n",
            "torch.Size([800])\n",
            "torch.Size([1, 800])\n",
            "torch.Size([1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIMHNv8PGbpD",
        "colab_type": "text"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xv5ANwkE_ZQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr = 0.001  # learning rate\n",
        "epochs = 250  # how many epochs to train for\n",
        "bs = 10  # batch size\n",
        "\n",
        "model = Mnist_mlp_classifier()\n",
        "\n",
        "y_pred = model.forward(x_test)\n",
        "print('Test error of model after training: ', loss_func(y_pred, y_test))\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  for i in range((n - 1) // bs + 1):\n",
        "    start_i = i * bs\n",
        "    end_i = start_i + bs\n",
        "    xb = x_train[start_i:end_i]\n",
        "    yb = y_train[start_i:end_i]\n",
        "    pred = model(xb)\n",
        "    loss = loss_func(pred, yb)\n",
        "    \n",
        "    loss.backward()\n",
        "    with torch.no_grad():\n",
        "      for p in model.parameters():\n",
        "        p -= p.grad * lr\n",
        "      model.zero_grad()\n",
        "      \n",
        "  print('Loss at epoch %d : %f' % (epoch, loss))\n",
        "\n",
        "y_pred = model(x_test)\n",
        "print('Test error of model after training: ', loss_func(y_pred, y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBccBOAKEE2_",
        "colab_type": "text"
      },
      "source": [
        "### Refactor using Dataset and DataLoader\n",
        "\n",
        "The Dataset and DataLoader classes provide us with a convinient way of batching and iterating through out datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Je0c6ybiEf2z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import TensorDataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "lr = 0.001  # learning rate\n",
        "epochs = 250  # how many epochs to train for\n",
        "bs = 10  # batch size\n",
        "\n",
        "train_dataset = TensorDataset(x_train, y_train)\n",
        "train_data_loader = DataLoader(train_dataset, batch_size=bs)\n",
        "\n",
        "model = Mnist_mlp_classifier()\n",
        "\n",
        "y_pred = model.forward(x_test)\n",
        "print('Test error of model after training: ', loss_func(y_pred, y_test))\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  for xb, yb in train_data_loader:\n",
        "    \n",
        "    pred = model(xb)\n",
        "    loss = loss_func(pred, yb)\n",
        "    \n",
        "    loss.backward()\n",
        "    with torch.no_grad():\n",
        "      for p in model.parameters():\n",
        "        p -= p.grad * lr\n",
        "      model.zero_grad()\n",
        "      \n",
        "  print('Loss at epoch %d : %f' % (epoch, loss))\n",
        "\n",
        "y_pred = model(x_test)\n",
        "print('Test error of model after training: ', loss_func(y_pred, y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lXEi_9G34xG",
        "colab_type": "text"
      },
      "source": [
        "### Refactor using optim\n",
        "------------------------------\n",
        "\n",
        "Pytorch also has a package with various optimization algorithms, ``torch.optim``.\n",
        "We can use the ``step`` method from our optimizer to take a forward step, instead\n",
        "of manually updating each parameter.\n",
        "\n",
        "This will let us replace our previous manually coded optimization step:\n",
        "::\n",
        "  with torch.no_grad():\n",
        "      for p in model.parameters(): p -= p.grad * lr\n",
        "      model.zero_grad()\n",
        "\n",
        "and instead use just:\n",
        "::\n",
        "  opt.step()\n",
        "  opt.zero_grad()\n",
        "\n",
        "(``optim.zero_grad()`` resets the gradient to 0 and we need to call it before\n",
        "computing the gradient for the next minibatch.)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6IWQmfyUyLy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch import optim\n",
        "from torch.utils.data import TensorDataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "lr = 0.001  # learning rate\n",
        "epochs = 250  # how many epochs to train for\n",
        "bs = 10  # batch size\n",
        "\n",
        "train_dataset = TensorDataset(x_train, y_train)\n",
        "train_data_loader = DataLoader(train_dataset, batch_size=bs)\n",
        "\n",
        "model = Mnist_mlp_classifier()\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
        "\n",
        "y_pred = model.forward(x_test)\n",
        "print('Test error of model after training: ', loss_func(y_pred, y_test))\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  for xb, yb in train_data_loader:\n",
        "    \n",
        "    pred = model.forward(xb)\n",
        "    loss = loss_func(pred, yb)\n",
        "    \n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "      \n",
        "  print('Loss at epoch %d : %f' % (epoch, loss))\n",
        "\n",
        "y_pred = model.forward(x_test)\n",
        "print('Test error of model after training: ', loss_func(y_pred, y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uTwW_1MPOpc",
        "colab_type": "text"
      },
      "source": [
        "## Visualizing the trained model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCglmDQ_N2Fg",
        "colab_type": "code",
        "outputId": "67ec40e9-5ea0-40a7-c2e3-19de8e344ebb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        }
      },
      "source": [
        "x = torch.linspace(-2, 2, 50).unsqueeze(1)\n",
        "y_ = model.forward(x)\n",
        "y = f(x)\n",
        "plt.plot(x_train.numpy(), y_train.detach().numpy(), 'o')\n",
        "plt.plot(x_test.numpy(), y_test.detach().numpy(), 'ro')\n",
        "plt.plot(x.numpy(), y.detach().numpy(), 'k-')\n",
        "plt.plot(x.numpy(), y_.detach().numpy(), 'b-')\n",
        "plt.legend(['Training data', 'Test data', 'Predicted function', 'True function'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1374: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fafd53538d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nHgUAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngWMzHmBGprA",
        "colab_type": "text"
      },
      "source": [
        "# Whole code in one place"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEJu7cX-Gsy7",
        "colab_type": "code",
        "outputId": "fa5e235a-1d6e-4b69-9735-e7b012cac5bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "from torch.utils.data import TensorDataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# params\n",
        "lr = 0.001  # learning rate\n",
        "epochs = 250  # how many epochs to train for\n",
        "bs = 10  # batch size\n",
        "\n",
        "# data processing\n",
        "def f(x):\n",
        "  return (x - 0.2)*(x + 0.3)*(3*x - 1.5) + 10\n",
        "\n",
        "SIZE = 50\n",
        "x = np.linspace(-2, 2, SIZE, np.float32)\n",
        "y = f(x)\n",
        "\n",
        "y_noise = y + 2 * np.random.randn(SIZE)\n",
        "\n",
        "idx = list(range(SIZE))\n",
        "np.random.shuffle(idx)\n",
        "\n",
        "x_train, y_train, x_test, y_test = x[idx][:40], y_noise[idx][:40], x[idx][40:], y_noise[idx][40:]\n",
        "\n",
        "x_train, y_train, x_test, y_test = map(\n",
        "    torch.FloatTensor, (x_train[:, np.newaxis], y_train[:, np.newaxis], x_test[:, np.newaxis], y_test[:, np.newaxis])\n",
        ")\n",
        "n, _ = x_train.shape\n",
        "\n",
        "train_dataset = TensorDataset(x_train, y_train)\n",
        "train_data_loader = DataLoader(train_dataset, batch_size=bs)\n",
        "\n",
        "# model definition\n",
        "class mlp_regreessor(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.layer1 = nn.Linear(1, 800)\n",
        "    self.layer2 = nn.Linear(800, 1)\n",
        "\n",
        "  def forward(self, xb):\n",
        "    h =  self.layer1(xb)\n",
        "    h = F.relu(h)\n",
        "    h =  self.layer2(h)\n",
        "    out = h\n",
        "    return out\n",
        "  \n",
        "model = mlp_regreessor()\n",
        "\n",
        "# loss function\n",
        "loss_func = F.mse_loss\n",
        "\n",
        "# optimizer\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
        "\n",
        "# training loop\n",
        "for epoch in range(epochs):\n",
        "  for xb, yb in train_data_loader:\n",
        "    \n",
        "    pred = model.forward(xb)\n",
        "    loss = loss_func(pred, yb)\n",
        "    \n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "      \n",
        "  print('Loss at epoch %d : %f' % (epoch, loss))\n",
        "\n",
        "# model evaluation\n",
        "y_pred = model.forward(x_test)\n",
        "print('Test error of model after training: ', loss_func(y_pred, y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss at epoch 0 : 29.780958\n",
            "Loss at epoch 1 : 19.800341\n",
            "Loss at epoch 2 : 18.759584\n",
            "Loss at epoch 3 : 17.916088\n",
            "Loss at epoch 4 : 16.934116\n",
            "Loss at epoch 5 : 15.963359\n",
            "Loss at epoch 6 : 15.069337\n",
            "Loss at epoch 7 : 14.266337\n",
            "Loss at epoch 8 : 13.550096\n",
            "Loss at epoch 9 : 12.911761\n",
            "Loss at epoch 10 : 12.342286\n",
            "Loss at epoch 11 : 11.832714\n",
            "Loss at epoch 12 : 11.375507\n",
            "Loss at epoch 13 : 10.964307\n",
            "Loss at epoch 14 : 10.593369\n",
            "Loss at epoch 15 : 10.257432\n",
            "Loss at epoch 16 : 9.952083\n",
            "Loss at epoch 17 : 9.673658\n",
            "Loss at epoch 18 : 9.418772\n",
            "Loss at epoch 19 : 9.184479\n",
            "Loss at epoch 20 : 8.968196\n",
            "Loss at epoch 21 : 8.768126\n",
            "Loss at epoch 22 : 8.582006\n",
            "Loss at epoch 23 : 8.408387\n",
            "Loss at epoch 24 : 8.245583\n",
            "Loss at epoch 25 : 8.092406\n",
            "Loss at epoch 26 : 7.947877\n",
            "Loss at epoch 27 : 7.811456\n",
            "Loss at epoch 28 : 7.681782\n",
            "Loss at epoch 29 : 7.558469\n",
            "Loss at epoch 30 : 7.440487\n",
            "Loss at epoch 31 : 7.328116\n",
            "Loss at epoch 32 : 7.220215\n",
            "Loss at epoch 33 : 7.116581\n",
            "Loss at epoch 34 : 7.016252\n",
            "Loss at epoch 35 : 6.920163\n",
            "Loss at epoch 36 : 6.826826\n",
            "Loss at epoch 37 : 6.736948\n",
            "Loss at epoch 38 : 6.649662\n",
            "Loss at epoch 39 : 6.564893\n",
            "Loss at epoch 40 : 6.482803\n",
            "Loss at epoch 41 : 6.403089\n",
            "Loss at epoch 42 : 6.325577\n",
            "Loss at epoch 43 : 6.250113\n",
            "Loss at epoch 44 : 6.176798\n",
            "Loss at epoch 45 : 6.105335\n",
            "Loss at epoch 46 : 6.035367\n",
            "Loss at epoch 47 : 5.967834\n",
            "Loss at epoch 48 : 5.901580\n",
            "Loss at epoch 49 : 5.836953\n",
            "Loss at epoch 50 : 5.773876\n",
            "Loss at epoch 51 : 5.712623\n",
            "Loss at epoch 52 : 5.652291\n",
            "Loss at epoch 53 : 5.594198\n",
            "Loss at epoch 54 : 5.536953\n",
            "Loss at epoch 55 : 5.481160\n",
            "Loss at epoch 56 : 5.426959\n",
            "Loss at epoch 57 : 5.374154\n",
            "Loss at epoch 58 : 5.321644\n",
            "Loss at epoch 59 : 5.271254\n",
            "Loss at epoch 60 : 5.222122\n",
            "Loss at epoch 61 : 5.174407\n",
            "Loss at epoch 62 : 5.126967\n",
            "Loss at epoch 63 : 5.081183\n",
            "Loss at epoch 64 : 5.036409\n",
            "Loss at epoch 65 : 4.993018\n",
            "Loss at epoch 66 : 4.949723\n",
            "Loss at epoch 67 : 4.908062\n",
            "Loss at epoch 68 : 4.867614\n",
            "Loss at epoch 69 : 4.828501\n",
            "Loss at epoch 70 : 4.789434\n",
            "Loss at epoch 71 : 4.751831\n",
            "Loss at epoch 72 : 4.714987\n",
            "Loss at epoch 73 : 4.679580\n",
            "Loss at epoch 74 : 4.644412\n",
            "Loss at epoch 75 : 4.610104\n",
            "Loss at epoch 76 : 4.577169\n",
            "Loss at epoch 77 : 4.545063\n",
            "Loss at epoch 78 : 4.513444\n",
            "Loss at epoch 79 : 4.482517\n",
            "Loss at epoch 80 : 4.452730\n",
            "Loss at epoch 81 : 4.423747\n",
            "Loss at epoch 82 : 4.395440\n",
            "Loss at epoch 83 : 4.367467\n",
            "Loss at epoch 84 : 4.340501\n",
            "Loss at epoch 85 : 4.314668\n",
            "Loss at epoch 86 : 4.289415\n",
            "Loss at epoch 87 : 4.264249\n",
            "Loss at epoch 88 : 4.240199\n",
            "Loss at epoch 89 : 4.216689\n",
            "Loss at epoch 90 : 4.194174\n",
            "Loss at epoch 91 : 4.171754\n",
            "Loss at epoch 92 : 4.149967\n",
            "Loss at epoch 93 : 4.129088\n",
            "Loss at epoch 94 : 4.108900\n",
            "Loss at epoch 95 : 4.089204\n",
            "Loss at epoch 96 : 4.069374\n",
            "Loss at epoch 97 : 4.050953\n",
            "Loss at epoch 98 : 4.032693\n",
            "Loss at epoch 99 : 4.015263\n",
            "Loss at epoch 100 : 3.998115\n",
            "Loss at epoch 101 : 3.981609\n",
            "Loss at epoch 102 : 3.965480\n",
            "Loss at epoch 103 : 3.950019\n",
            "Loss at epoch 104 : 3.935272\n",
            "Loss at epoch 105 : 3.920456\n",
            "Loss at epoch 106 : 3.906137\n",
            "Loss at epoch 107 : 3.892546\n",
            "Loss at epoch 108 : 3.879589\n",
            "Loss at epoch 109 : 3.866922\n",
            "Loss at epoch 110 : 3.854149\n",
            "Loss at epoch 111 : 3.842248\n",
            "Loss at epoch 112 : 3.830676\n",
            "Loss at epoch 113 : 3.819589\n",
            "Loss at epoch 114 : 3.808879\n",
            "Loss at epoch 115 : 3.798200\n",
            "Loss at epoch 116 : 3.787684\n",
            "Loss at epoch 117 : 3.778183\n",
            "Loss at epoch 118 : 3.768711\n",
            "Loss at epoch 119 : 3.759929\n",
            "Loss at epoch 120 : 3.750906\n",
            "Loss at epoch 121 : 3.742238\n",
            "Loss at epoch 122 : 3.734147\n",
            "Loss at epoch 123 : 3.726303\n",
            "Loss at epoch 124 : 3.719020\n",
            "Loss at epoch 125 : 3.711676\n",
            "Loss at epoch 126 : 3.704486\n",
            "Loss at epoch 127 : 3.697525\n",
            "Loss at epoch 128 : 3.691053\n",
            "Loss at epoch 129 : 3.685045\n",
            "Loss at epoch 130 : 3.679095\n",
            "Loss at epoch 131 : 3.673056\n",
            "Loss at epoch 132 : 3.667308\n",
            "Loss at epoch 133 : 3.661986\n",
            "Loss at epoch 134 : 3.657002\n",
            "Loss at epoch 135 : 3.652229\n",
            "Loss at epoch 136 : 3.647600\n",
            "Loss at epoch 137 : 3.642737\n",
            "Loss at epoch 138 : 3.638322\n",
            "Loss at epoch 139 : 3.634420\n",
            "Loss at epoch 140 : 3.630449\n",
            "Loss at epoch 141 : 3.627030\n",
            "Loss at epoch 142 : 3.623500\n",
            "Loss at epoch 143 : 3.619750\n",
            "Loss at epoch 144 : 3.616405\n",
            "Loss at epoch 145 : 3.613522\n",
            "Loss at epoch 146 : 3.610740\n",
            "Loss at epoch 147 : 3.608278\n",
            "Loss at epoch 148 : 3.605757\n",
            "Loss at epoch 149 : 3.603007\n",
            "Loss at epoch 150 : 3.600814\n",
            "Loss at epoch 151 : 3.598615\n",
            "Loss at epoch 152 : 3.596784\n",
            "Loss at epoch 153 : 3.595013\n",
            "Loss at epoch 154 : 3.593405\n",
            "Loss at epoch 155 : 3.591630\n",
            "Loss at epoch 156 : 3.590282\n",
            "Loss at epoch 157 : 3.588776\n",
            "Loss at epoch 158 : 3.587618\n",
            "Loss at epoch 159 : 3.586648\n",
            "Loss at epoch 160 : 3.585784\n",
            "Loss at epoch 161 : 3.584768\n",
            "Loss at epoch 162 : 3.583773\n",
            "Loss at epoch 163 : 3.582954\n",
            "Loss at epoch 164 : 3.582120\n",
            "Loss at epoch 165 : 3.581658\n",
            "Loss at epoch 166 : 3.581266\n",
            "Loss at epoch 167 : 3.580975\n",
            "Loss at epoch 168 : 3.580768\n",
            "Loss at epoch 169 : 3.580269\n",
            "Loss at epoch 170 : 3.580317\n",
            "Loss at epoch 171 : 3.579981\n",
            "Loss at epoch 172 : 3.579997\n",
            "Loss at epoch 173 : 3.580190\n",
            "Loss at epoch 174 : 3.580391\n",
            "Loss at epoch 175 : 3.580384\n",
            "Loss at epoch 176 : 3.580423\n",
            "Loss at epoch 177 : 3.580673\n",
            "Loss at epoch 178 : 3.580712\n",
            "Loss at epoch 179 : 3.581065\n",
            "Loss at epoch 180 : 3.581427\n",
            "Loss at epoch 181 : 3.582078\n",
            "Loss at epoch 182 : 3.582631\n",
            "Loss at epoch 183 : 3.583100\n",
            "Loss at epoch 184 : 3.583377\n",
            "Loss at epoch 185 : 3.584144\n",
            "Loss at epoch 186 : 3.584496\n",
            "Loss at epoch 187 : 3.585348\n",
            "Loss at epoch 188 : 3.586016\n",
            "Loss at epoch 189 : 3.586811\n",
            "Loss at epoch 190 : 3.587601\n",
            "Loss at epoch 191 : 3.588326\n",
            "Loss at epoch 192 : 3.588979\n",
            "Loss at epoch 193 : 3.589696\n",
            "Loss at epoch 194 : 3.590372\n",
            "Loss at epoch 195 : 3.591363\n",
            "Loss at epoch 196 : 3.592242\n",
            "Loss at epoch 197 : 3.593216\n",
            "Loss at epoch 198 : 3.594183\n",
            "Loss at epoch 199 : 3.595080\n",
            "Loss at epoch 200 : 3.595860\n",
            "Loss at epoch 201 : 3.596851\n",
            "Loss at epoch 202 : 3.597693\n",
            "Loss at epoch 203 : 3.598655\n",
            "Loss at epoch 204 : 3.599714\n",
            "Loss at epoch 205 : 3.600704\n",
            "Loss at epoch 206 : 3.601728\n",
            "Loss at epoch 207 : 3.602771\n",
            "Loss at epoch 208 : 3.603753\n",
            "Loss at epoch 209 : 3.604653\n",
            "Loss at epoch 210 : 3.605731\n",
            "Loss at epoch 211 : 3.606671\n",
            "Loss at epoch 212 : 3.607663\n",
            "Loss at epoch 213 : 3.608805\n",
            "Loss at epoch 214 : 3.609809\n",
            "Loss at epoch 215 : 3.610845\n",
            "Loss at epoch 216 : 3.611949\n",
            "Loss at epoch 217 : 3.612906\n",
            "Loss at epoch 218 : 3.613903\n",
            "Loss at epoch 219 : 3.614809\n",
            "Loss at epoch 220 : 3.615962\n",
            "Loss at epoch 221 : 3.616690\n",
            "Loss at epoch 222 : 3.617786\n",
            "Loss at epoch 223 : 3.618933\n",
            "Loss at epoch 224 : 3.619790\n",
            "Loss at epoch 225 : 3.620829\n",
            "Loss at epoch 226 : 3.621880\n",
            "Loss at epoch 227 : 3.622802\n",
            "Loss at epoch 228 : 3.623708\n",
            "Loss at epoch 229 : 3.624613\n",
            "Loss at epoch 230 : 3.625759\n",
            "Loss at epoch 231 : 3.626451\n",
            "Loss at epoch 232 : 3.627521\n",
            "Loss at epoch 233 : 3.628442\n",
            "Loss at epoch 234 : 3.629358\n",
            "Loss at epoch 235 : 3.630346\n",
            "Loss at epoch 236 : 3.631344\n",
            "Loss at epoch 237 : 3.632181\n",
            "Loss at epoch 238 : 3.633100\n",
            "Loss at epoch 239 : 3.633887\n",
            "Loss at epoch 240 : 3.634781\n",
            "Loss at epoch 241 : 3.635680\n",
            "Loss at epoch 242 : 3.636304\n",
            "Loss at epoch 243 : 3.637285\n",
            "Loss at epoch 244 : 3.638148\n",
            "Loss at epoch 245 : 3.638854\n",
            "Loss at epoch 246 : 3.639792\n",
            "Loss at epoch 247 : 3.640703\n",
            "Loss at epoch 248 : 3.641461\n",
            "Loss at epoch 249 : 3.642267\n",
            "Test error of model after training:  tensor(4.8146, grad_fn=<MseLossBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvFAD7aWLhy-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}